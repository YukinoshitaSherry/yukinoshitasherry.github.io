---
title: 正则化为什么能缓解过拟合
date: 2024-03-03
categories:
  - 学AI/DS
tags:
  - ML
desc: 正则化为什么能缓解过拟合的数学原理与直觉理解。
---


## 背景

在线性模型中，我们通常使用如下形式拟合数据：

$$\mathbf{y} \approx \mathbf{X} \mathbf{w}$$

其中：
* $\mathbf{X} \in \mathbb{R}^{n \times d}$ 为特征矩阵，$n$ 个样本、$d$ 个特征
* $\mathbf{y} \in \mathbb{R}^n$ 为目标向量
* $\mathbf{w} \in \mathbb{R}^d$ 为待求权重

目标是最小化训练误差，最常见的方法是**最小二乘**（MSE）：

$$\min_{\mathbf{w}} | \mathbf{X}\mathbf{w} - \mathbf{y} |_2^2$$


当特征高度相关或样本量小、维度高时：
* $(\mathbf{X}^\top \mathbf{X})$ 可能接近奇异（行列式接近 0，条件数大）
* 这种情况下最小二乘解 $(\mathbf{w}^*)$ 的数值可能非常大
* 模型会对训练数据拟合很好，但对新样本泛化能力差

直观理解：模型在“拟合噪声”，而非真实规律。

<br>

## 无正则化最小二乘

### 损失函数

定义损失函数为训练样本的均方误差：

$$L(\mathbf{w}) = | \mathbf{X}\mathbf{w} - \mathbf{y} |_2^2 = (\mathbf{X}\mathbf{w}-\mathbf{y})^\top (\mathbf{X}\mathbf{w}-\mathbf{y})$$

展开得到二次型形式：

$$L(\mathbf{w}) = \mathbf{w}^\top \mathbf{X}^\top \mathbf{X} \mathbf{w} - 2 (\mathbf{X}^\top \mathbf{y})^\top \mathbf{w} + \mathbf{y}^\top \mathbf{y}$$

这一步体现了最小二乘问题的核心数学结构：损失是 **关于 $\mathbf{w}$ 的二次函数**。

<br>

### 矩阵求导规则

对列向量 $\mathbf{w}$：

1. $\frac{\partial}{\partial \mathbf{w}} (\mathbf{w}^\top \mathbf{A} \mathbf{w}) = (\mathbf{A} + \mathbf{A}^\top)\mathbf{w}$
   * 如果 $\mathbf{A}$ 对称，则简化为 $2 \mathbf{A} \mathbf{w}$
2. $\frac{\partial}{\partial \mathbf{w}} (\mathbf{b}^\top \mathbf{w}) = \mathbf{b}$
3. 对常数求导为 0

【类比：二次项导数为“线性项”，一次项导数为常量，常数项消失。】

<br>

### 梯度推导

对 $\mathbf{w}$ 求导：

$$\frac{\partial L}{\partial \mathbf{w}} = 2 \mathbf{X}^\top \mathbf{X} \mathbf{w} - 2 \mathbf{X}^\top \mathbf{y} = 2 \mathbf{X}^\top (\mathbf{X}\mathbf{w} - \mathbf{y})$$

* 直觉解释：梯度表示 当前预测 $\mathbf{X}\mathbf{w}$ 与真实标签 $\mathbf{y}$ 的误差在特征方向上的投影
* 所以梯度方向就是**沿误差最大的方向调整权重**

<br>

### 闭式解（正规方程）

令梯度为零：

$$\mathbf{X}^\top (\mathbf{X}\mathbf{w} - \mathbf{y}) = 0$$

得到最小二乘解：

$$\mathbf{w}^* = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$$

* 这就是 **正规方程**
* 条件：$\mathbf{X}^\top \mathbf{X}$ 可逆
* 否则，需要使用伪逆或正则化

<br>

### 数值不稳定示例

假设特征高度相关：

$$x_2 \approx x_1 \quad \Rightarrow \quad \mathbf{X}^\top \mathbf{X} \approx \left[\begin{array}{cc} |x_1|^2 & |x_1|^2 \\\\ |x_1|^2 & |x_1|^2 \end{array}\right]$$

* $\det(\mathbf{X}^\top \mathbf{X}) \approx 0$ → 矩阵接近奇异
* 最小二乘解 $\mathbf{w}^*$ 会非常大
* 直观理解：模型试图用大权重"精确拟合"几乎相同的特征列
这就是 **过拟合的数学根源**。



加入L2的示例：

$$\mathbf{X} = \left[\begin{array}{cc} 1 & 1.01 \\\\ 2 & 2.02 \\\\ 3 & 3.03 \end{array}\right]$$

$$\mathbf{y} = \left[\begin{array}{c} 2 \\\\ 4 \\\\ 6 \end{array}\right]$$

* **无正则化**：
  * $\mathbf{X}^\top \mathbf{X}$ 接近奇异
  * 权重 $\mathbf{w}^*$ 很大，过拟合明显

* **L2 正则化**（$\lambda = 0.1$）：
  * $\mathbf{X}^\top \mathbf{X} + \lambda I$ 可逆
  * 权重 $\mathbf{w}^*$ 较小且稳定

<br>

## 正则化方法

### L2 正则化（Ridge 回归）

#### 目标函数

$$\min_{\mathbf{w}} | \mathbf{X}\mathbf{w} - \mathbf{y} |_2^2 + \lambda |\mathbf{w}|_2^2$$

* $\lambda > 0$ 控制正则化强度
* $|\mathbf{w}|_2^2 = \sum_i w_i^2$ 惩罚极端权重

<br>

#### 梯度与闭式解

求导：

$$\frac{\partial}{\partial \mathbf{w}} \left( |\mathbf{X}\mathbf{w}-\mathbf{y}|_2^2 + \lambda |\mathbf{w}|_2^2 \right) = 2 \mathbf{X}^\top (\mathbf{X}\mathbf{w} - \mathbf{y}) + 2 \lambda \mathbf{w}$$

令梯度为 0：

$$\mathbf{X}^\top (\mathbf{X}\mathbf{w} - \mathbf{y}) + \lambda \mathbf{w} = 0$$

闭式解：

$$\mathbf{w}^*_{\text{ridge}} = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}$$

* 矩阵 $\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I}$ 总是可逆
* 抑制极端权重
* 增强数值稳定性和泛化能力

<br>

### L1 正则化（Lasso 回归）

目标函数：

$$\min_{\mathbf{w}} | \mathbf{X}\mathbf{w} - \mathbf{y} |_2^2 + \lambda |\mathbf{w}|_1$$

* $|\mathbf{w}|_1 = \sum_i |w_i|$
* 作用：产生稀疏解（部分 $w_i=0$）
* 优点：特征选择
* 缺点：没有闭式解，一般用坐标下降或凸优化求解

<br>

### 弹性网（Elastic Net）

结合 L1 和 L2 正则化：

$$\min_{\mathbf{w}} | \mathbf{X}\mathbf{w} - \mathbf{y} |_2^2 + \lambda_1 |\mathbf{w}|_1 + \lambda_2 |\mathbf{w}|_2^2$$

* 兼具稀疏性和数值稳定性
* 适合高维相关特征问题

<br>

### Dropout（神经网络正则化）

* 在训练神经网络时随机丢弃部分神经元
* 减少共适应性，提高泛化能力
* 本质：对每次迭代随机采样子网络 → 平滑权重更新

<br>

## 几何直觉

1. **无正则化**
   * 特征高度相关 → $\mathbf{X}^\top \mathbf{X}$ 条件数大
   * 权重沿特征相关方向被放大 → 极端权重 → 过拟合

2. **L2 正则化**
   * 等高线由长条 → 圆形
   * 权重更小、更平滑
   * 控制权重幅度

3. **L1 正则化**
   * 等高线为菱形
   * 权重沿边缘 → 部分权重为 0 → 稀疏解

4. **Elastic Net**
   * 兼具 L1/L2 优点 → 稀疏且平滑

5. **Dropout**
   * 模拟"集成学习"
   * 减少神经元间过度依赖


<br>

## 核心理解

* **过拟合数学本质**：
  $\mathbf{X}^\top \mathbf{X}$ 条件数大 → 极端权重 → 拟合噪声

* **正则化数学本质**：
  修改正规方程，矩阵稳定可逆 → 权重受限 → 泛化能力提升

* **梯度直觉**：
  * 无正则化：$\mathbf{w} \leftarrow \mathbf{w} - \eta \mathbf{X}^\top (\mathbf{X}\mathbf{w}-\mathbf{y})$
  * L2 正则化：$\mathbf{w} \leftarrow \mathbf{w} - \eta (\mathbf{X}^\top (\mathbf{X}\mathbf{w}-\mathbf{y}) + \lambda \mathbf{w})$
  * 梯度方向 = 误差在特征方向上的投影 + 权重惩罚

* **直观理解**：正则化约束权重，让模型"不要过分依赖某个方向"，从而减少噪声拟合。

<br>

## 在深度学习中的扩展理解

在神经网络中，过拟合问题比线性模型更严重，因为网络通常参数众多，模型表达能力很强。常用正则化手段包括 L2、Dropout 之外，还有以下几种理解与方法。

<br>

### 权重衰减（Weight Decay）

* 数学形式与 Ridge 回归类似：在梯度更新中加入 L2 项
* 对参数更新公式：

$$\mathbf{w} \leftarrow \mathbf{w} - \eta \left( \frac{\partial L}{\partial \mathbf{w}} + \lambda \mathbf{w} \right)$$

* 直觉理解：
  * 限制权重幅度，防止梯度爆炸
  * 减少训练过程对单个特征的过度依赖

* 在深度网络中，权重衰减对每一层都生效，使整个网络更加平滑。

<br>

### Dropout

* 在训练过程中，随机丢弃一部分神经元输出
* 数学本质：对前向传播输出 $h_i$ 乘以随机二值掩码 $r_i \sim \text{Bernoulli}(p)$
  $$\tilde{h}_i = r_i h_i$$
* 反向传播时只更新未被丢弃的神经元
* 直觉理解：每次训练时学习的都是**网络的不同子集** → 类似 ensemble → 提升泛化

<br>

### 早停（Early Stopping）

* 在训练过程中监控验证集损失
* 当验证集损失不再下降时停止训练
* 数学理解：防止训练过久导致训练误差过拟合 → 权重增长过大
* 与 L2 正则化类似，都是抑制极端权重，但通过训练迭代次数控制，而非显式惩罚项

<br>

### BatchNorm & LayerNorm 

* BatchNorm 将每一层激活归一化：
  $$\hat{h} = \frac{h - \mu_{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^2 + \epsilon}}$$
* 作用类似正则化：
  * 控制激活范围 → 防止梯度爆炸/消失
  * 对权重的敏感性降低
* 在训练中常被认为具有"隐式正则化"效果

<br>

### 总结(深度学习)

1. 所有正则化方法的核心目标：**防止权重极端增长，抑制噪声拟合**
2. 权重衰减 / L2：显式约束参数幅度
3. Dropout：随机约束子网络 → 平滑整体模型
4. 早停：控制训练迭代 → 限制权重生长
5. BatchNorm / LayerNorm：归一化激活 → 降低梯度敏感性

总体理解：深度网络中每一层都可能出现“局部过拟合”，正则化手段都是为了让整个网络输出更加稳定，泛化能力更强。

<br>

