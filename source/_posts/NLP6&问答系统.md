---
title: NLP(6)：问答系统
date: 2024-02-07 12:00
categories:
  - 学AI/DS
tags:
  - NLP
desc: CS224n Lec10 笔记，资料整合与一些自己的思考。阅读理解问答、开放域问答、知识库问答。

---

- 参考
    - <a href="https://www.showmeai.tech/tutorials/36">`showmeai-斯坦福CS224n教程`</a>

## 问答系统概述

问答系统(Question Answering, QA)是自然语言处理中的一个重要任务，其目标是从给定的文本中找出问题的答案。根据不同的应用场景，问答系统可以分为以下几种类型：

1. **开放域问答**：问题可以涉及任何领域，需要从大规模知识库中检索答案
   - 特点：需要处理海量数据，检索效率是关键
   - 应用：搜索引擎、智能助手
   - 挑战：答案质量、检索效率、知识覆盖度
   - 实现方法：
     - 基于检索的问答：先检索相关文档，再抽取答案
     - 基于生成的问答：直接生成答案
     - 混合方法：结合检索和生成

2. **封闭域问答**：问题限定在特定领域内
   - 特点：领域知识集中，答案质量高
   - 应用：客服系统、专业领域咨询
   - 挑战：领域适应性、知识更新
   - 实现方法：
     - 基于规则的系统
     - 基于模板的问答
     - 基于知识图谱的问答

3. **阅读理解式问答**：给定一段文本，从中找出问题的答案
   - 特点：答案必须来自给定文本
   - 应用：考试系统、文档问答
   - 挑战：答案抽取准确性、上下文理解
   - 实现方法：
     - 基于注意力机制的模型
     - 基于预训练语言模型的模型
     - 基于图神经网络的模型


4. **知识库问答**：从结构化的知识库中检索答案
   - 特点：答案来自结构化数据
   - 应用：知识图谱问答
   - 挑战：语义解析、知识表示
   - 实现方法：
     - 基于语义解析的方法
     - 基于信息抽取的方法
     - 基于神经符号的方法
<br>

## 阅读理解式问答
### 总览

阅读理解式问答是最基础的问答任务，其输入包括：
- 问题 $Q = (q_1, q_2, ..., q_m)$，其中 $q_i$ 表示问题的第 $i$ 个词
- 上下文段落 $P = (p_1, p_2, ..., p_n)$，其中 $p_i$ 表示段落的第 $i$ 个词
- 答案 $A$（可能是文本片段或单个词）

### 主要数据集

1. **SQuAD (Stanford Question Answering Dataset)**
   - 包含10万+问答对
   - 答案必须是段落中的文本片段
   - 评估指标：精确匹配(EM)和F1分数
   - 特点：
     - 问题由人工标注者根据段落生成
     - 答案必须是段落中的连续文本片段
     - 每个问题都有多个标注者验证
   - 优点：
     - 数据质量高，标注规范：每个问题都经过多个标注者验证，确保答案的准确性
     - 问题类型多样：包含事实型、推理型、比较型等多种问题类型
     - 评估指标明确：使用EM和F1分数，便于模型比较和评估
   - 缺点：
     - 答案必须是原文片段，限制了生成能力：系统无法生成新的答案，只能从原文中抽取
     - 得分高的系统并不能真正理解人类语言：系统没有真正了解一切，仍然在做一种匹配问题
       - 原因：系统主要依赖模式匹配和统计特征，而不是真正的语义理解
       - 表现：在对抗样本测试中，系统容易被误导
       - 影响：限制了系统在实际应用中的可靠性
     - 缺乏多跳推理问题：大多数问题只需要单步推理，无法测试系统的复杂推理能力
       - 单跳推理：直接从文本中找出答案
       - 多跳推理：需要结合多个信息片段进行推理
       - 影响：无法评估系统的深层理解能力

2. **MS MARCO**
   - 更接近真实场景的问答数据
   - 答案不一定是段落中的原文
   - 特点：
     - 问题来自真实的搜索引擎查询：反映了真实用户的信息需求
     - 答案可以是生成的文本：支持更灵活的答案形式
     - 包含多文档检索任务：需要从多个文档中综合信息
   - 优点：
     - 更接近真实应用场景：问题来自真实用户，答案形式更灵活
     - 支持生成式答案：可以生成新的答案，不限于原文片段
     - 包含多文档信息：需要综合多个文档的信息，更接近实际应用
   - 缺点：
     - 答案质量参差不齐：由于答案可以是生成的，质量难以保证
     - 评估难度大：需要人工评估答案的质量和相关性
     - 数据规模相对较小：相比SQuAD，数据量较少

3. **TriviaQA**
   - 基于维基百科的问答数据
   - 问题来自真实的问答网站
   - 特点：
     - 问题更接近真实用户查询：反映了真实用户的知识需求
     - 答案可能出现在多个文档中：需要综合多个文档的信息
     - 包含实体链接任务：需要识别和链接实体
   - 优点：
     - 问题更自然：问题来自真实用户，更接近实际应用
     - 支持多文档答案：可以综合多个文档的信息
     - 包含实体知识：需要理解实体关系
   - 缺点：
     - 答案可能不完整：由于答案来自多个文档，可能不完整
     - 文档质量不稳定：维基百科文档质量参差不齐
     - 评估标准不统一：缺乏统一的评估标准

4. **HotpotQA**
   - 多跳推理问答数据集
   - 特点：
     - 需要多个推理步骤：问题需要多步推理才能得到答案
     - 包含支持事实标注：提供了推理的依据
     - 问题类型多样：包含多种复杂推理问题
   - 优点：
     - 支持复杂推理：可以测试系统的深层推理能力
     - 提供推理依据：有助于理解系统的推理过程
     - 评估更全面：可以评估系统的推理能力
   - 缺点：
     - 数据规模较小：由于标注难度大，数据量较少
     - 标注成本高：需要标注推理过程和依据
     - 模型训练困难：需要更复杂的模型架构

5. **Natural Questions**
   - 基于Google搜索的问答数据
   - 特点：
     - 问题来自真实搜索：反映了真实用户的信息需求
     - 包含长答案和短答案：支持多种答案形式
     - 基于维基百科：数据质量较高
   - 优点：
     - 数据规模大：包含大量真实问题
     - 问题自然度高：问题来自真实用户
     - 答案类型多样：支持多种答案形式
   - 缺点：
     - 答案质量不稳定：由于答案来自维基百科，质量可能不稳定
     - 评估标准复杂：需要评估答案的完整性和相关性
     - 数据分布不均衡：某些类型的问题可能较少

### 模型架构

#### 基础模型

最基础的阅读理解模型包含以下组件：

1. **编码层**：将问题和段落转换为向量表示
   - 词嵌入：$E(Q) = [e_{q1}, e_{q2}, ..., e_{qm}]$，其中 $e_{qi} \in \mathbb{R}^d$
     - 作用：将词转换为向量表示
     - 实现：可以使用预训练词向量或随机初始化
   - 问题编码：$H^Q = \text{BiLSTM}(E(Q))$，其中 $H^Q \in \mathbb{R}^{m \times 2h}$
     - 前向LSTM：$h_t^f = \text{LSTM}\_f(h\_{t-1}^f, e_t)$
       - 作用：捕获前文信息
       - 实现：使用LSTM单元
     - 后向LSTM：$h_t^b = \text{LSTM}\_b(h\_{t+1}^b, e_t)$
       - 作用：捕获后文信息
       - 实现：使用LSTM单元
     - 双向拼接：$h_t = [h_t^f; h_t^b]$
       - 作用：结合前后文信息
       - 实现：向量拼接
   - 段落编码：$H^P = \text{BiLSTM}(E(P))$，其中 $H^P \in \mathbb{R}^{n \times 2h}$
     - 使用与问题编码相同的BiLSTM结构
     - 共享参数可以减少模型复杂度
   - 优点：
     - 结构简单，易于实现：模型架构清晰，易于理解和实现
     - 计算效率高：使用BiLSTM，计算效率较高
     - 训练稳定：模型结构简单，训练过程稳定
   - 缺点：
     - 特征提取能力有限：BiLSTM的特征提取能力有限
     - 长距离依赖处理能力弱：难以处理长文本
     - 缺乏预训练知识：没有利用预训练语言模型的知识

2. **注意力层**：计算问题和段落之间的注意力
   - 注意力矩阵：$A = H^Q(H^P)^T$，其中 $A \in \mathbb{R}^{m \times n}$
     - 每个元素 $A_{ij}$ 表示问题词 $i$ 和段落词 $j$ 的相似度
     - 作用：计算问题和段落之间的相关性
     - 实现：矩阵乘法
   - 问题感知的段落表示：$H^{P2Q} = \text{softmax}(A)H^Q$
     - 对每个段落词，计算其与所有问题词的加权和
     - 权重通过softmax归一化：$\alpha\_{ij} = \frac{\exp(A\_{ij})}{\sum_k \exp(A_{ik})}$
     - 作用：将问题信息融入段落表示
     - 实现：注意力加权
   - 优点：
     - 能够捕获问题和段落的相关性：通过注意力机制计算相关性
     - 提供可解释的注意力权重：可以分析模型的关注点
     - 计算效率高：注意力计算效率较高
   - 缺点：
     - 注意力机制可能不够精确：简单的点积注意力可能不够精确
     - 缺乏多层次的注意力：没有考虑不同层次的语义信息
     - 难以处理复杂推理：难以处理需要多步推理的问题

3. **输出层**：预测答案的起始和结束位置
   - 起始位置概率：$P_s = \text{softmax}(W_s[H^P; H^{P2Q}])$
     - $W_s \in \mathbb{R}^{1 \times 4h}$ 是学习参数
     - $[H^P; H^{P2Q}]$ 表示拼接操作
     - 作用：预测答案的起始位置
     - 实现：线性层+softmax
   - 结束位置概率：$P_e = \text{softmax}(W_e[H^P; H^{P2Q}])$
     - 使用独立的参数 $W_e$
     - 确保结束位置在起始位置之后
     - 作用：预测答案的结束位置
     - 实现：线性层+softmax
   - 优点：
     - 直接预测答案位置：模型输出直观
     - 训练目标明确：使用交叉熵损失
     - 实现简单：只需要两个线性层
   - 缺点：
     - 只能抽取连续文本片段：无法处理不连续的答案
     - 无法生成新答案：只能从原文中抽取
     - 对答案长度敏感：难以处理长答案

#### 改进模型

1. **BiDAF (Bidirectional Attention Flow)**-ICLR 2017
   - **核心思想是 the Attention Flow layer**
   - 双向注意力流
   - 多层次的注意力机制
   - 模型架构：
     - 字符级编码层：
       - 字符嵌入：$E_{char}(w) = [e_{c1}, e_{c2}, ..., e_{cl}]$，其中 $e_{ci} \in \mathbb{R}^{d_{char}}$
       - 字符CNN：$h_{char} = \text{CNN}(E_{char}(w))$，其中 $h_{char} \in \mathbb{R}^{d_{char}}$
       - 作用：捕获词的形态学特征
       - 实现：使用CNN提取字符级特征
     - 词级编码层：
       - 词嵌入：$E_{word}(w) = [e_{w1}, e_{w2}, ..., e_{wn}]$，其中 $e_{wi} \in \mathbb{R}^{d_{word}}$
       - 词向量：$h_{word} = [E_{word}(w); h_{char}]$，其中 $h_{word} \in \mathbb{R}^{d_{word} + d_{char}}$
       - 作用：结合词和字符级特征
       - 实现：向量拼接
     - 上下文编码层：
       - 问题编码：$H^Q = \text{BiLSTM}(h\_{word}^Q)$，其中 $H^Q \in \mathbb{R}^{m \times 2h}$
       - 段落编码：$H^P = \text{BiLSTM}(h\_{word}^P)$，其中 $H^P \in \mathbb{R}^{n \times 2h}$
       - 作用：捕获上下文信息
       - 实现：使用BiLSTM编码
   - 注意力机制：
     - 相似度矩阵：$S_{ij} = w^T[h_i^P; h_j^Q; h_i^P \circ h_j^Q]$
       - $w \in \mathbb{R}^{6h}$ 是学习参数
       - $\circ$ 表示元素级乘法
       - 作用：计算问题和段落之间的相似度
       - 实现：线性层+元素级乘法
     - 上下文到问题的注意力：$c2q_i = \sum_j \alpha\_{ij}h_j^Q$
       - $\alpha\_{ij} = \frac{\exp(S_{ij})}{\sum_k \exp(S_{ik})}$
       - 作用：将问题信息融入段落表示
       - 实现：注意力加权
     - 问题到上下文的注意力：$q2c_i = \sum_j \beta\_{ij}h\_j^P$
       - $\beta\_{ij} = \frac{\exp(\max\_k S\_{ik})}{\sum\_l \exp(\max\_k S\_{lk})}$
       - 作用：将段落信息融入问题表示
       - 实现：注意力加权
   - 建模层：
     - 输入：$G = [H^P; H^{P2Q}; H^P \circ H^{P2Q}; H^P \circ H^{Q2P}]$
     - 输出：$M = \text{BiLSTM}(G)$，其中 $M \in \mathbb{R}^{n \times 2h}$
     - 作用：融合所有信息
     - 实现：BiLSTM编码
   - 输出层：
     - 起始位置：$P_s = \text{softmax}(W_s[M; G])$
     - 结束位置：$P_e = \text{softmax}(W_e[M; G])$
     - 作用：预测答案位置
     - 实现：线性层+softmax
   - 模型优势：
     - 双向注意力提供更丰富的上下文信息：同时考虑问题和段落的信息
     - 多层次注意力捕获不同粒度的语义关系：可以处理不同层次的语义信息
   - 优点：
     - 注意力机制更完善：使用双向和多层次注意力
     - 特征提取能力更强：可以捕获更丰富的语义信息
     - 推理能力更好：可以处理更复杂的问题
   - 缺点：
     - 计算复杂度高：注意力计算复杂度高
     - 训练不稳定：模型结构复杂，训练不稳定
     - 需要更多数据：需要大量数据才能训练好

2. **BERT-based模型**
   - 使用预训练语言模型
   - 将问题和段落拼接后输入BERT
   - 输出层预测答案位置
   - 具体实现：
     - 输入格式：[CLS] 问题 [SEP] 段落 [SEP]
       - 作用：区分问题和段落
       - 实现：特殊标记拼接
     - 输入嵌入：
       - 词嵌入：$E_{word}(w) \in \mathbb{R}^{d_{word}}$
       - 位置嵌入：$E_{pos}(p) \in \mathbb{R}^{d_{pos}}$
       - 段落嵌入：$E_{seg}(s) \in \mathbb{R}^{d_{seg}}$
       - 最终嵌入：$E = E_{word} + E_{pos} + E_{seg}$
       - 作用：结合词、位置和段落信息
       - 实现：向量加法
     - 自注意力机制：
       - 查询矩阵：$Q = E W^Q$，其中 $W^Q \in \mathbb{R}^{d \times d_k}$
       - 键矩阵：$K = E W^K$，其中 $W^K \in \mathbb{R}^{d \times d_k}$
       - 值矩阵：$V = E W^V$，其中 $W^V \in \mathbb{R}^{d \times d_v}$
       - 注意力分数：$A = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
       - 作用：计算序列内部的注意力
       - 实现：矩阵乘法+softmax
     - 前馈网络：
       - 第一层：$FFN_1(x) = \max(0, xW_1 + b_1)$
       - 第二层：$FFN_2(x) = xW_2 + b_2$
       - 作用：非线性变换
       - 实现：线性层+ReLU
     - 使用BERT的最后一层隐藏状态
       - 作用：获取上下文表示
       - 实现：BERT编码
     - 通过两个线性层预测起始和结束位置
       - 作用：预测答案位置
       - 实现：线性层+softmax
   - 优势：
     - 利用预训练知识：使用预训练语言模型的知识
     - 更好的上下文理解能力：可以理解更复杂的语义
     - 端到端训练更简单：模型结构简单
   - 优点：
     - 性能最好：在多个数据集上取得最好结果
     - 泛化能力强：可以处理各种类型的问题
     - 训练效率高：端到端训练效率高
   - 缺点：
     - 计算资源需求大：需要大量计算资源
     - 可解释性差：模型结构复杂，难以解释
     - 需要大量预训练数据：需要大量数据预训练

3. **RoBERTa-based模型**
   - BERT的改进版本
   - 特点：
     - 更大的训练数据
     - 更长的训练时间
     - 动态掩码机制
   - 具体改进：
     - 动态掩码：
       - 每次前向传播时重新生成掩码
       - 掩码概率：$p_{mask} = 0.15$
       - 作用：增加模型的鲁棒性
       - 实现：随机掩码生成
     - 更大的批次大小：
       - 批次大小：$batch\_size = 8K$
       - 作用：提高训练效率
       - 实现：梯度累积
     - 更长的序列长度：
       - 最大序列长度：$max\_len = 512$
       - 作用：处理更长文本
       - 实现：位置编码扩展
   - 优点：
     - 性能优于BERT：在多个任务上超过BERT
     - 训练更稳定：使用动态掩码，训练更稳定
     - 泛化能力更强：可以处理更多类型的问题
   - 缺点：
     - 训练成本更高：需要更多计算资源
     - 模型体积更大：模型参数更多
     - 推理速度较慢：推理时间更长

4. **ALBERT模型**
   - 轻量级BERT变体
   - 特点：
     - 参数共享：共享参数减少模型体积
     - 层间参数分解：分解参数减少参数量
     - 句子顺序预测：使用句子顺序预测任务
   - 具体实现：
     - 参数共享：
       - 所有层共享相同的参数
       - 参数量减少：$N \times d \rightarrow d$，其中 $N$ 是层数
       - 作用：减少模型体积
       - 实现：参数复用
     - 层间参数分解：
       - 词嵌入：$E \in \mathbb{R}^{V \times E}$
       - 隐藏层：$H \in \mathbb{R}^{E \times H}$
       - 分解为：$E \in \mathbb{R}^{V \times E}$ 和 $H \in \mathbb{R}^{E \times H}$
       - 作用：减少参数量
       - 实现：矩阵分解
     - 句子顺序预测：
       - 输入：两个句子 $[s_1, s_2]$
       - 输出：预测 $s_1$ 和 $s_2$ 的顺序
       - 作用：学习句子间关系
       - 实现：二分类任务
   - 优点：
     - 模型体积小：参数量少
     - 训练速度快：训练时间短
     - 内存占用少：内存需求小
   - 缺点：
     - 性能略低于BERT：性能有所下降
     - 特征提取能力较弱：特征提取能力有限
     - 不适合复杂任务：难以处理复杂问题

### 评估指标

1. **精确匹配(Exact Match, EM)**
   - 预测答案与标准答案完全匹配的比例
   - 计算公式：$EM = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \mathbb{I}(A_i = \hat{A}_i)$
     - $\mathbb{I}$ 是指示函数
     - $A_i$ 是标准答案
     - $\hat{A}_i$ 是预测答案
   - 优点：直观、易于理解
   - 缺点：过于严格，不考虑部分正确的情况

2. **F1分数**
   - 基于预测答案和标准答案的词重叠计算
   - 计算公式：
     - 精确率：$P = \frac{|A \cap \hat{A}|}{|A|}$
     - 召回率：$R = \frac{|A \cap \hat{A}|}{|\hat{A}|}$
     - F1：$F1 = \frac{2 \times P \times R}{P + R}$
   - 优点：考虑部分匹配
   - 缺点：不考虑词序和语义


<br>

## 开放域问答


### 总览

开放域问答需要从大规模文档集合中检索相关文档，然后从中提取答案。主要挑战包括：

1. **文档检索**
   - 使用传统IR方法（如BM25）
     - 基于词频和逆文档频率
     - 计算公式：$score(d,q) = \sum_{t \in q} \frac{tf(t,d) \times idf(t)}{k_1 + tf(t,d)}$
   - 使用神经检索模型（如DPR）
     - 双塔结构：问题和文档分别编码
     - 相似度计算：$s(q,d) = \cos(E_q(q), E_d(d))$

2. **答案生成**
   - 抽取式方法：从文档中抽取答案片段
     - 优点：答案更准确
     - 缺点：受限于文档内容
   - 生成式方法：生成新的答案文本
     - 优点：答案更自然
     - 缺点：可能产生幻觉

### 检索增强生成(RAG)

RAG结合了检索和生成的优势：

1. **检索阶段**
   - 使用问题检索相关文档
   - 计算公式：$P(d|q) = \frac{\exp(s(q,d))}{\sum_{d'} \exp(s(q,d'))}$
     - $s(q,d)$ 是问题和文档的相似度
     - 使用softmax归一化得到文档概率

2. **生成阶段**
   - 将检索到的文档和问题一起输入生成模型
   - 生成答案：$P(a|q,d) = \prod_{t=1}^T P(a_t|a_{<t},q,d)$
     - $a_t$ 是第 $t$ 个生成的词
     - $a_{<t}$ 是已生成的词序列
   - 优势：
     - 结合检索的准确性和生成的灵活性
     - 减少幻觉问题
     - 提高答案的可解释性

<br>

## 知识库问答

知识库问答从结构化的知识库中检索答案，主要方法包括：

1. **语义解析**
   - 将自然语言问题转换为逻辑形式
     - 例如：SPARQL查询
     - 使用语法分析树
   - 在知识库中执行查询
     - 处理实体链接
     - 处理关系映射

2. **信息抽取**
   - 从问题中抽取实体和关系
     - 实体识别：$P(e|q) = \text{softmax}(W_e h_q)$
     - 关系抽取：$P(r|q) = \text{softmax}(W_r h_q)$
   - 在知识库中匹配答案
     - 实体链接：$P(e'|e) = \frac{\exp(s(e,e'))}{\sum_{e''} \exp(s(e,e''))}$
     - 路径查找：使用图算法

### 评估方法

1. **人工评估**
   - 评估答案的准确性
     - 语义正确性
     - 完整性
   - 评估答案的完整性
     - 信息覆盖度
     - 冗余度

2. **自动评估**
   - 基于标准答案的匹配
     - 精确匹配
     - 语义相似度
   - 基于知识库的验证
     - 事实一致性
     - 逻辑正确性

<br>

## 未来发展方向

1. **多跳推理**
   - 需要多个推理步骤才能得到答案
   - 挑战：
     - 保持推理的连贯性
     - 提高可解释性
     - 处理长距离依赖
   - 解决方法：
     - 使用图神经网络
     - 引入中间推理步骤
     - 设计可解释的推理路径

2. **多模态问答**
   - 结合文本、图像、视频等多种模态
   - 挑战：
     - 跨模态信息的对齐
     - 模态间的语义融合
     - 处理模态缺失
   - 解决方法：
     - 使用多模态预训练模型
     - 设计跨模态注意力机制
     - 引入模态特定的编码器

3. **可解释性**
   - 提供答案的推理过程
   - 提高模型的可信度
   - 方法：
     - 注意力可视化
     - 推理路径生成
     - 反事实解释

4. **低资源场景**
   - 减少对大规模标注数据的依赖
   - 提高模型的泛化能力
   - 方法：
     - 迁移学习
     - 少样本学习
     - 自监督学习