---
title: NLP(8)：子词模型、指代消解问题、文本生成任务
date: 2024-02-08 
categories:
  - 学AI/DS
tags:
  - NLP
desc: CS224n Lec12&16&15 笔记，资料整合与一些自己的思考。

---

- 参考
    - <a href="https://www.showmeai.tech/tutorials/36">`showmeai-斯坦福CS224n教程`</a>
    - <a href="https://www.showmeai.tech/article-detail/249">`ShowMeAI-子词模型与指代消解`</a>

## 子词模型
**Subword Models**

### 为什么需要？

在自然语言处理中，我们经常遇到以下问题：
- 词表大小问题：语言中的词汇量非常大，如果使用词级别的模型，词表会变得非常庞大
  - 例如：英语词典通常包含超过100,000个词
  - 专业领域（如医学、法律）可能有更多专业术语
  - 新词不断产生（如网络用语、新科技词汇）
- 未知词（OOV）问题：测试集中可能出现训练集中没有的词
  - 例如：训练集中有"running"，但测试集出现"jogging"
  - 新出现的网络用语或流行语
  - 专业术语或罕见词
- 词形变化问题：同一个词可能有多种形式（如：run, runs, running）
  - 时态变化：walk, walked, walking
  - 单复数变化：book, books
  - 词性变化：happy, happiness, happily

子词模型通过将词分解成更小的单元来解决这些问题。

### 常见的子词模型

#### Byte Pair Encoding (BPE)
BPE是一种数据压缩算法，在NLP中用于构建子词单元。其工作流程：

1. 初始化：将所有词分解成字符序列
   - 例如："low" → "l o w"
   - 每个字符作为一个基本单元

2. 迭代：
   - 统计所有相邻字符对的出现频率
   - 将最频繁出现的字符对合并成一个新的子词单元
   - 更新词表

示例：
```
原始词：low, lower, newest, widest
初始字符：l o w, l o w e r, n e w e s t, w i d e s t
第一次合并（最常见的对）：l o w, l o w e r, n e w e s t, w i d e s t
（假设"es"最常见）
第二次合并：l o w, l o w e r, n e w es t, w i d es t
```

BPE的优势：
- 可以处理未见过的词
- 能够捕捉常见的词根和词缀
- 词表大小可控

#### WordPiece
WordPiece是BERT等模型使用的子词模型，与BPE的主要区别：
- 使用语言模型分数而不是频率来决定合并
  - 计算合并后的语言模型似然度
  - 选择使似然度最大的合并
- 使用"##"标记表示子词是词的一部分
  - 帮助模型识别子词的位置
  - 区分词首和词中/词尾

示例：
```
原始词：playing
WordPiece分解：play ##ing

其他例子：
- "unhappiness" → "un ##happ ##iness"
- "international" → "inter ##nation ##al"
```

WordPiece的特点：
- 更适合处理形态丰富的语言
- 能够更好地处理前缀和后缀
- 在预训练语言模型中表现优异

### 优势

1. 更好的泛化能力：
   - 可以处理未见过的词
     - 例如：即使没见过"unhappiness"，也能通过"un"、"happy"、"ness"理解
   - 能够学习到词根、词缀等语言特征
     - 例如：理解"un-"表示否定，"-ness"表示名词
   - 有助于处理新词和罕见词

2. 更小的词表：
   - 减少内存使用
     - 词表大小通常可以减少50-80%
     - 例如：从100,000词减少到30,000子词
   - 加快训练和推理速度
     - 更少的参数需要更新
     - 更快的查找速度

3. 更好的语义表示：
   - 相似的词会有相似的子词表示
     - 例如："running"和"jogging"可能共享"ing"子词
   - 有助于捕捉词义关系
     - 词根相同的词可能具有相关含义
     - 词缀可以表示词性和语法功能

## 指代消解
**Coreference Resolution**

### 概念

指代消解是识别文本中指向同一实体的不同表达的任务。例如：
```
小明昨天买了一本书。他很喜欢它。
```
这里"他"指代"小明"，"它"指代"书"。

指代消解的类型：
1. 代词指代：
   - 人称代词：他、她、它
   - 指示代词：这、那
   - 关系代词：谁、什么

2. 名词指代：
   - 同义表达：总统、国家元首
   - 上位词：狗、宠物
   - 部分指代：车轮、车

3. 零指代：
   - 省略主语：去吃饭了（省略了"我"）
   - 省略宾语：把书放在桌子上（省略了"它"）

### 挑战

1. 语言歧义：
   - 代词可能指向多个可能的先行词
     - 例如："小明和小红去公园，他玩得很开心"
     - 需要根据上下文判断"他"指代谁
   - 上下文信息的重要性
     - 需要理解整个文档的语义
     - 需要考虑说话者的意图

2. 长距离依赖：
   - 指代词和先行词可能相距很远
     - 可能跨多个段落
     - 可能跨多个文档
   - 需要理解整个文档的上下文
     - 需要保持长期记忆
     - 需要处理文档结构

3. 隐式指代：
   - 零指代（省略主语）
     - 常见于对话和口语
     - 需要根据上下文补充
   - 部分指代
     - 例如："车轮"指代"车"
     - 需要理解部分-整体关系

### 现代指代消解方法

#### 基于神经网络的端到端模型

1. 特征提取：
   - 词嵌入
     - 使用预训练语言模型（如BERT）获取词向量
     - 捕捉词的语义信息
   - 上下文表示
     - 使用双向LSTM或Transformer编码上下文
     - 考虑词的上下文信息
   - 句法特征
     - 词性标注
     - 依存句法分析
     - 命名实体识别

2. 指代对评分：
   - 计算每个可能的指代对之间的分数
     - 使用注意力机制计算相关性
     - 考虑语义相似度
   - 使用注意力机制捕捉长距离依赖
     - 自注意力：捕捉文档内部关系
     - 交叉注意力：连接不同部分的信息

3. 聚类：
   - 将高分的指代对聚类成指代链
     - 使用贪心算法
     - 使用全局优化
   - 使用贪心算法或全局优化
     - 考虑所有可能的指代关系
     - 优化整体指代链的质量

示例：
```
输入：小明昨天买了一本书。他很喜欢它。
处理步骤：
1. 识别可能的指代词：他、它
2. 识别可能的先行词：小明、书
3. 计算指代对分数：
   - (他, 小明): 0.9
   - (他, 书): 0.1
   - (它, 小明): 0.2
   - (它, 书): 0.8
4. 形成指代链：
   - 链1: {小明, 他}
   - 链2: {书, 它}
```

评估指标：
1. 精确率（Precision）：
   - 正确识别的指代对数量
   - 除以系统识别的所有指代对数量

2. 召回率（Recall）：
   - 正确识别的指代对数量
   - 除以所有正确的指代对数量

3. F1分数：
   - 精确率和召回率的调和平均
   - 综合评估系统性能
