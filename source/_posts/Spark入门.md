---
title: Sparkä½¿ç”¨æ•™ç¨‹
date: 2025-10-09
categories: 
    - å­¦CS/SE
tags: 
    - DS
desc: 2025~2026ç§‹å†¬å­¦å­¦æœŸå¤§æ•°æ®åˆ†æä¸åº”ç”¨è¯¾ç¨‹éœ€è¦ä½¿ç”¨Sparkï¼Œä¸€ä¸ªåŸºäºå†…å­˜çš„åˆ†å¸ƒå¼å¤§æ•°æ®è®¡ç®—æ¡†æ¶ï¼Œæ”¯æŒæ‰¹å¤„ç†ã€æµå¤„ç†ã€SQL æŸ¥è¯¢ã€æœºå™¨å­¦ä¹ å’Œå›¾è®¡ç®—ã€‚
---



## å‚è€ƒèµ„æ–™

* å®˜æ–¹ç½‘ç«™ï¼š[https://spark.apache.org/](https://spark.apache.org/)
* å®˜æ–¹æ–‡æ¡£ï¼š[https://spark.apache.org/docs/latest/](https://spark.apache.org/docs/latest/)
* Spark æºç ä»“åº“ï¼š[https://github.com/apache/spark](https://github.com/apache/spark)
* å­¦ä¹ èµ„æºï¼š
  * ã€ŠLearning Spark, 2nd Editionã€‹â€” Oâ€™Reilly
  * Databricks Spark æ•™ç¨‹
  

##  ç®€ä»‹

**Apache Spark** æ˜¯ä¸€ä¸ªå¼€æºçš„ã€åŸºäºå†…å­˜ï¼ˆIn-Memoryï¼‰çš„åˆ†å¸ƒå¼å¤§æ•°æ®è®¡ç®—æ¡†æ¶ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„ MapReduceï¼ŒSpark é€šè¿‡å†…å­˜è®¡ç®—æ˜¾è‘—æå‡äº†æ‰¹å¤„ç†å’Œè¿­ä»£è®¡ç®—çš„æ€§èƒ½ï¼ŒåŒæ—¶æ”¯æŒæµå¤„ç†ã€SQL æŸ¥è¯¢ã€æœºå™¨å­¦ä¹ å’Œå›¾è®¡ç®—ç­‰å¤šç§æ•°æ®å¤„ç†æ¨¡å¼ã€‚

**ç‰¹ç‚¹ï¼š**

* **é«˜æ€§èƒ½**ï¼šSpark åˆ©ç”¨å†…å­˜è®¡ç®—åŠ å¿«ä»»åŠ¡å¤„ç†é€Ÿåº¦ï¼Œå¹¶æä¾› DAG è°ƒåº¦ä¼˜åŒ–ã€‚
* **å¤šè¯­è¨€æ”¯æŒ**ï¼šæä¾› **Scala**ã€**Java**ã€**Python (PySpark)**ã€**R** æ¥å£ã€‚
* **å¤šç§è¿è¡Œæ¨¡å¼**ï¼š

  * æœ¬åœ°æ¨¡å¼ï¼ˆLocal Modeï¼‰ï¼šå•æœºå¤šçº¿ç¨‹ã€‚
  * Standalone æ¨¡å¼ï¼šSpark è‡ªå¸¦çš„é›†ç¾¤ç®¡ç†ã€‚
  * YARN æ¨¡å¼ï¼šåœ¨ Hadoop YARN ä¸Šè¿è¡Œã€‚
  * Kubernetes (K8s) æ¨¡å¼ï¼šå®¹å™¨åŒ–éƒ¨ç½²ã€‚
* **ç»Ÿä¸€è®¡ç®—å¼•æ“**ï¼šåŒæ—¶æ”¯æŒæ‰¹å¤„ç†ï¼ˆBatchï¼‰ã€æµå¤„ç†ï¼ˆStreamï¼‰ã€äº¤äº’å¼æŸ¥è¯¢ï¼ˆSQLï¼‰ã€æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œå›¾è®¡ç®—ï¼ˆGraphï¼‰ç­‰ä»»åŠ¡ã€‚
* **æ˜“æ‰©å±•æ€§**ï¼šå¯æ°´å¹³æ‰©å±•é›†ç¾¤è§„æ¨¡ï¼Œä»å‡ å°æœºå™¨åˆ°ä¸Šåƒå°èŠ‚ç‚¹ã€‚
* **ä¸°å¯Œç”Ÿæ€**ï¼šä¸ Hadoopã€Kafkaã€Hiveã€HBase ç­‰æ— ç¼é›†æˆã€‚

<br>

### æ ¸å¿ƒæ¦‚å¿µ


#### RDDï¼ˆResilient Distributed Datasetï¼‰

##### æ¦‚å¿µ
* Spark çš„åº•å±‚æŠ½è±¡ï¼Œä¸å¯å˜çš„åˆ†å¸ƒå¼æ•°æ®é›†åˆã€‚
ç®€è€Œè¨€ä¹‹ï¼ŒRDD å°±æ˜¯ä¸€ä¸ªåˆ†å¸ƒåœ¨é›†ç¾¤ä¸­å¤šä¸ªèŠ‚ç‚¹ä¸Šçš„å¯¹è±¡é›†åˆï¼Œä½ å¯ä»¥åœ¨ä¸Šé¢æ‰§è¡Œå„ç§å¹¶è¡Œè®¡ç®—ï¼ˆæ¯”å¦‚ mapã€filterã€reduceï¼‰ã€‚

* **ç‰¹æ€§**ï¼š
  * **å¼¹æ€§ï¼ˆResilientï¼‰**ï¼šè‡ªåŠ¨å¤„ç†èŠ‚ç‚¹æ•…éšœã€‚
  * **åˆ†å¸ƒå¼ï¼ˆDistributedï¼‰**ï¼šæ•°æ®åˆ†å¸ƒåœ¨é›†ç¾¤èŠ‚ç‚¹ä¸Šã€‚
  * **æ•°æ®é›†ï¼ˆDatasetï¼‰**ï¼šå¯æ‰§è¡Œå¹¶è¡Œæ“ä½œã€‚
* **æ“ä½œç±»å‹**ï¼š
  * **Transformationï¼ˆè½¬æ¢ï¼‰**ï¼šå¦‚ `map`ã€`filter`ã€`flatMap`ï¼Œæƒ°æ€§æ‰§è¡Œã€‚
  * **Actionï¼ˆè¡ŒåŠ¨ï¼‰**ï¼šå¦‚ `count`ã€`collect`ï¼Œè§¦å‘è®¡ç®—ã€‚

##### RDDå¯¹è±¡çš„ç»„æˆç»“æ„

ä¸€ä¸ª RDD å¯¹è±¡å†…éƒ¨åŒ…å«ä»¥ä¸‹å…³é”®å±æ€§ï¼š
| å±æ€§                            | è¯´æ˜                               |
| ----------------------------- | -------------------------------- |
| **åˆ†åŒºï¼ˆPartitionsï¼‰**            | RDD è¢«åˆ’åˆ†ä¸ºå¤šä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºå¯åœ¨ä¸åŒèŠ‚ç‚¹å¹¶è¡Œå¤„ç†      |
| **ä¾èµ–å…³ç³»ï¼ˆDependenciesï¼‰**        | è®°å½•å½“å‰ RDD æ˜¯ç”±å“ªäº› RDD è½¬æ¢è€Œæ¥ï¼ˆç”¨äºå®¹é”™æ¢å¤ï¼‰   |
| **è®¡ç®—å‡½æ•°ï¼ˆCompute Functionï¼‰**    | æ¯ä¸ªåˆ†åŒºçš„æ•°æ®å¦‚ä½•è¢«è®¡ç®—ï¼ˆmap/filter ç­‰ï¼‰       |
| **åˆ†åŒºå™¨ï¼ˆPartitionerï¼‰**          | å¯é€‰ï¼Œç”¨äºå†³å®šé”®å€¼å¯¹ RDD çš„åˆ†åŒºè§„åˆ™             |
| **å­˜å‚¨ä½ç½®ï¼ˆPreferred Locationsï¼‰** | æç¤º Spark å“ªäº›èŠ‚ç‚¹å·²æœ‰æ•°æ®å‰¯æœ¬ï¼ˆæé«˜ localityï¼‰ |


##### RDDåˆ›å»ºæ–¹å¼
1ï¸âƒ£ ä»å¤–éƒ¨æ•°æ®åˆ›å»º

```
# ä¾‹å­ï¼šä»æœ¬åœ°æˆ–HDFSæ–‡ä»¶ä¸­åˆ›å»º
rdd = spark.sparkContext.textFile("hdfs://path/data.txt")
```

2ï¸âƒ£ ä»é›†åˆåˆ›å»º
```
data = [1, 2, 3, 4, 5]
rdd = spark.sparkContext.parallelize(data)
```

```
3ï¸âƒ£ ä»å…¶ä»– RDD è½¬æ¢å¾—åˆ°
rdd2 = rdd.map(lambda x: x * 2)
rdd3 = rdd2.filter(lambda x: x > 5)
```

#### DataFrame ä¸ Dataset

* **DataFrame**ï¼šä»¥åˆ—ä¸ºåŸºç¡€çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç±»ä¼¼ SQL è¡¨æ ¼ã€‚
* **Dataset**ï¼šç»“åˆ RDD çš„ç±»å‹å®‰å…¨å’Œ DataFrame çš„ä¼˜åŒ–ç‰¹æ€§ï¼ˆä¸»è¦åœ¨ Scala/Java ä¸­ï¼‰ã€‚
* **ä¼˜ç‚¹**ï¼š

  * Catalyst ä¼˜åŒ–å™¨ï¼šè‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆæ‰§è¡Œè®¡åˆ’ã€‚
  * Tungsten å†…å­˜ç®¡ç†ï¼šæå‡å†…å­˜å’Œ CPU ä½¿ç”¨æ•ˆç‡ã€‚
  * SQL å…¼å®¹ï¼šæ”¯æŒæ ‡å‡† SQL æŸ¥è¯¢ã€‚

<br>


### æ ¸å¿ƒç»„ä»¶

#### Spark Core

* **åŠŸèƒ½**ï¼šä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†ã€æ•…éšœæ¢å¤ã€åˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œã€‚
* **æ ¸å¿ƒæ¦‚å¿µ**ï¼š

  * **Driver**ï¼šè´Ÿè´£è°ƒåº¦ä»»åŠ¡å’Œç»´æŠ¤åº”ç”¨çŠ¶æ€ã€‚
  * **Executor**ï¼šåœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šæ‰§è¡Œä»»åŠ¡ï¼Œç®¡ç†æ•°æ®å’Œè®¡ç®—ã€‚
  * **Task**ï¼šæ‰§è¡Œå•ä¸ªè®¡ç®—å•å…ƒã€‚
  * **Job**ï¼šç”¨æˆ·æäº¤çš„è®¡ç®—ä»»åŠ¡ã€‚
  * **Stage**ï¼šä»»åŠ¡åˆ’åˆ†ä¸ºå¤šä¸ªé˜¶æ®µï¼ŒStage å†…éƒ¨ä»»åŠ¡å¯ä»¥å¹¶è¡Œæ‰§è¡Œã€‚
* **æ•°æ®æŠ½è±¡**ï¼šRDD æ˜¯ Spark Core çš„åŸºç¡€ï¼Œæ‰€æœ‰é«˜çº§ API éƒ½æ˜¯åŸºäº RDD æ„å»ºçš„ã€‚

#### Spark SQL

* **åŠŸèƒ½**ï¼š

  * æ‰§è¡Œç»“æ„åŒ–æ•°æ®æŸ¥è¯¢ã€‚
  * æä¾› DataFrameã€Dataset APIã€‚
  * æ”¯æŒæ ‡å‡† SQL æŸ¥è¯¢è¯­å¥ã€‚
  * æ”¯æŒ Hive è¡¨å’Œ Parquetã€ORC ç­‰å¤šç§æ–‡ä»¶æ ¼å¼ã€‚
* **ç‰¹ç‚¹**ï¼š

  * Catalyst ä¼˜åŒ–å™¨ï¼šè‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ã€‚
  * ä¸ BI å·¥å…·é›†æˆï¼Œä¾¿äºåšæŠ¥è¡¨å’Œåˆ†æã€‚
* **ç¤ºä¾‹**ï¼š

  ```python
  from pyspark.sql import SparkSession

  spark = SparkSession.builder.appName("SparkSQLExample").getOrCreate()
  df = spark.read.json("people.json")
  df.show()
  df.createOrReplaceTempView("people")
  spark.sql("SELECT name, age FROM people WHERE age > 20").show()
  ```

#### Spark Streaming

* **åŠŸèƒ½**ï¼šå¤„ç†å®æ—¶æµæ•°æ®ï¼Œå¦‚æ—¥å¿—ã€æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆKafkaï¼‰ç­‰ã€‚
* **å¤„ç†æ–¹å¼**ï¼š

  * **DStreamï¼ˆç¦»æ•£æµï¼‰**ï¼šå°†å®æ—¶æ•°æ®åˆ‡åˆ†ä¸ºå¾®æ‰¹ï¼ˆMicro-Batchï¼‰ã€‚
  * æ”¯æŒçª—å£æ“ä½œã€çŠ¶æ€ç®¡ç†ã€‚
* **é›†æˆ**ï¼š

  * å¯ä»¥ä¸ Kafkaã€Flumeã€Kinesis ç­‰æ¶ˆæ¯é˜Ÿåˆ—ç»“åˆã€‚
  * è¾“å‡ºç»“æœå¯å†™å…¥ HDFSã€æ•°æ®åº“æˆ–å¤–éƒ¨ç³»ç»Ÿã€‚

#### MLlibï¼ˆæœºå™¨å­¦ä¹ åº“ï¼‰

* **åŠŸèƒ½**ï¼š

  * æä¾›å¸¸ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼šåˆ†ç±»ã€å›å½’ã€èšç±»ã€ååŒè¿‡æ»¤ç­‰ã€‚
  * æä¾›ç‰¹å¾å·¥ç¨‹å·¥å…·ï¼šç‰¹å¾æå–ã€è½¬æ¢ã€é€‰æ‹©ã€æ ‡å‡†åŒ–ç­‰ã€‚
  * æ”¯æŒç®¡é“ï¼ˆPipelineï¼‰ç®¡ç†ã€‚
* **ç‰¹ç‚¹**ï¼š

  * åˆ†å¸ƒå¼è®¡ç®—èƒ½åŠ›å¼ºï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®è®­ç»ƒã€‚
  * ä¸ DataFrame API ç´§å¯†ç»“åˆï¼Œæ–¹ä¾¿é›†æˆæ•°æ®å¤„ç†å’Œæ¨¡å‹è®­ç»ƒã€‚

#### GraphXï¼ˆå›¾è®¡ç®—åº“ï¼‰

* **åŠŸèƒ½**ï¼š

  * åˆ†å¸ƒå¼å›¾è®¡ç®—æ¡†æ¶ï¼Œæ”¯æŒå›¾çš„æ„å»ºã€è®¡ç®—å’ŒæŸ¥è¯¢ã€‚
  * æä¾› PageRankã€Connected Components ç­‰å›¾ç®—æ³•ã€‚
* **ç‰¹ç‚¹**ï¼š

  * RDD åŸºç¡€æ„å»ºï¼Œæ”¯æŒå›¾çš„å¹¶è¡Œå¤„ç†ã€‚
  * å¯ç»“åˆ Spark SQL å’Œ MLlib åšå¤æ‚åˆ†æã€‚

<br>

### å·¥ä½œæµç¨‹


<img src="https://pic1.zhimg.com/v2-70c9f2c758e31aca524394ba69c27504_r.jpg" style="width:90%"><br>
<img src="https://img2.baidu.com/it/u=3994114859,2070865283&fm=253&fmt=auto&app=138&f=JPEG?w=813&h=500" style="width:80%"><br>



#### å·¥ä½œåŸç†

Apache Spark çš„æ ¸å¿ƒæ˜¯ **åŸºäºå†…å­˜çš„åˆ†å¸ƒå¼è®¡ç®—å¼•æ“**ã€‚å…¶æ‰§è¡Œé€»è¾‘å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **åº”ç”¨æäº¤ï¼ˆApplication Submissionï¼‰**
   ç”¨æˆ·é€šè¿‡ `spark-submit` æˆ– Spark Shell æäº¤ä½œä¸šï¼ˆApplicationï¼‰ï¼ŒæŒ‡å®šä½œä¸šçš„ä»£ç å’Œä¾èµ–ã€‚

2. **Driver ç¨‹åº**

   * Driver æ˜¯ Spark åº”ç”¨çš„æ§åˆ¶ä¸­å¿ƒï¼Œè´Ÿè´£ï¼š

     * åˆ›å»º **SparkContext / SparkSession**
     * ç»´æŠ¤åº”ç”¨çŠ¶æ€
     * å°†ç”¨æˆ·ä»£ç è½¬æ¢æˆè®¡ç®—ä»»åŠ¡
     * ä¸é›†ç¾¤ç®¡ç†å™¨é€šä¿¡ç”³è¯·èµ„æº

3. **é›†ç¾¤ç®¡ç†å™¨ï¼ˆCluster Managerï¼‰**

   * Spark æ”¯æŒå¤šç§é›†ç¾¤ç®¡ç†æ¨¡å¼ï¼šStandaloneã€YARNã€Mesosã€Kubernetesã€‚
   * é›†ç¾¤ç®¡ç†å™¨è´Ÿè´£åˆ†é… **Executor** èµ„æºï¼ˆå·¥ä½œèŠ‚ç‚¹ä¸Šçš„ JVM è¿›ç¨‹ï¼‰ã€‚

4. **Executorï¼ˆæ‰§è¡Œå™¨ï¼‰**

   * Executor æ˜¯åœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šè¿è¡Œçš„è¿›ç¨‹ï¼š

     * æ‰§è¡Œä»»åŠ¡ï¼ˆTaskï¼‰
     * å­˜å‚¨è®¡ç®—ç»“æœå’Œä¸­é—´æ•°æ®
     * ä¸ Driver é€šä¿¡åé¦ˆçŠ¶æ€

5. **ä»»åŠ¡ï¼ˆTaskï¼‰ä¸é˜¶æ®µï¼ˆStageï¼‰**

   * Driver å°†ä½œä¸šï¼ˆJobï¼‰åˆ’åˆ†ä¸ºä¸€ä¸ªæˆ–å¤šä¸ª **Stage**ã€‚
   * Stage å†…éƒ¨åŒ…å«å¤šä¸ª **Task**ï¼Œå¯åœ¨é›†ç¾¤èŠ‚ç‚¹å¹¶è¡Œæ‰§è¡Œã€‚
   * **Stage åˆ’åˆ†ä¾æ®**ï¼šRDD çš„ **çª„ä¾èµ–ï¼ˆNarrow Dependencyï¼‰** ä¸ **å®½ä¾èµ–ï¼ˆWide Dependencyï¼‰**

     * çª„ä¾èµ–ï¼šä¸€ä¸ª partition åªä¾èµ–ä¸Šæ¸¸ä¸€ä¸ª partition â†’ å¯å¹¶è¡Œè®¡ç®—
     * å®½ä¾èµ–ï¼šä¸€ä¸ª partition ä¾èµ–ä¸Šæ¸¸å¤šä¸ª partition â†’ éœ€è¦ Shuffle


#### æ‰§è¡Œé€»è¾‘æµç¨‹

ä¸‹é¢ä»¥ DataFrame/RDD ä¸ºä¾‹è¯´æ˜ Spark çš„è®¡ç®—æµç¨‹ï¼š

##### æ­¥éª¤ 1ï¼šåˆ›å»ºæ•°æ®æº

* ä» HDFSã€Hiveã€JSONã€Parquet ç­‰è¯»å–æ•°æ®

```python
df = spark.read.json("people.json")
```

* æ­¤æ—¶åªæ˜¯åˆ›å»º **é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼ˆLogical Planï¼‰**ï¼Œè¿˜æ²¡æœ‰çœŸæ­£è®¡ç®—ã€‚


##### æ­¥éª¤ 2ï¼šè½¬æ¢æ“ä½œï¼ˆTransformationï¼‰

* ç”¨æˆ·å¯¹ RDD/DataFrame åš map/filter/join ç­‰æ“ä½œ

```python
adults = df.filter(df.age > 18)
```

* **ç‰¹ç‚¹**ï¼š

  * æƒ°æ€§æ‰§è¡Œï¼ˆLazy Evaluationï¼‰ï¼šä¸ä¼šç«‹å³è®¡ç®—
  * Spark ä¼šæŠŠæ‰€æœ‰ Transformation æ„å»ºæˆ **DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰**



##### æ­¥éª¤ 3ï¼šè¡ŒåŠ¨æ“ä½œï¼ˆActionï¼‰

* ç”¨æˆ·è§¦å‘ collectã€countã€show ç­‰æ“ä½œ

```python
adults.show()
```

* **ç‰¹ç‚¹**ï¼š

  * æ‰ä¼šè§¦å‘ DAG çš„ **ç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼ˆPhysical Planï¼‰**
  * Driver æ ¹æ® DAG åˆ’åˆ† Stage
  * ç”Ÿæˆ Task å‘é€ç»™ Executor



##### æ­¥éª¤ 4ï¼šShuffle & ä»»åŠ¡è°ƒåº¦

* **çª„ä¾èµ–**ï¼šTask å¯ç‹¬ç«‹æ‰§è¡Œï¼Œä¸éœ€è¦æ•°æ®é‡åˆ†åŒº
* **å®½ä¾èµ–**ï¼šTask éœ€è¦ Shuffleï¼ŒExecutor ä¼šå°†ä¸­é—´æ•°æ®å†™å…¥ç£ç›˜/ç½‘ç»œä¼ è¾“
* **Task è°ƒåº¦**ï¼š

  1. Driver æŒ‰ Stage æ‹†åˆ† Task
  2. Task åˆ†å‘åˆ°å„ Executor æ‰§è¡Œ
  3. Executor æ‰§è¡Œè®¡ç®—å¹¶è¿”å›ç»“æœç»™ Driver



##### æ­¥éª¤ 5ï¼šç»“æœè¿”å›

* Action å®Œæˆåï¼ŒDriver æ”¶é›†ç»“æœï¼š

  * å†™å…¥å¤–éƒ¨å­˜å‚¨ï¼ˆHDFSã€æ•°æ®åº“ã€Kafkaï¼‰
  * è¿”å›ç»™ç”¨æˆ·ï¼ˆcollect/showï¼‰



#### å†…éƒ¨é€»è¾‘ç»“æ„

| å±‚æ¬¡                     | ä½œç”¨            | ç¤ºä¾‹ç»„ä»¶                                  |
| ---------------------- | ------------- | ------------------------------------- |
| **åº”ç”¨å±‚ Application**    | ç”¨æˆ·æäº¤ Spark ç¨‹åº | Driverã€SparkContext                   |
| **æ ¸å¿ƒè°ƒåº¦ Core**          | ä»»åŠ¡åˆ’åˆ†ã€DAG è°ƒåº¦   | DAG Schedulerã€Task Scheduler          |
| **æ‰§è¡Œå±‚ Executor**       | å…·ä½“è®¡ç®—æ‰§è¡Œã€å­˜å‚¨ä¸­é—´æ•°æ® | Taskã€Shuffleã€Block Manager            |
| **æ•°æ®æŠ½è±¡ RDD/DataFrame** | åˆ†å¸ƒå¼æ•°æ®ç®¡ç†ã€è½¬æ¢é€»è¾‘  | RDDã€DataFrameã€Dataset                 |
| **è®¡ç®—ä¼˜åŒ–**               | æé«˜æ€§èƒ½ã€å‡å°‘ I/O   | Catalyst Optimizerã€Tungsten Execution |



#### æ•°æ®æµç¤ºæ„å›¾

```
ç”¨æˆ·ä»£ç  (Driver) 
       â”‚
       â–¼
åˆ›å»º RDD/DataFrame ï¼ˆé€»è¾‘è®¡åˆ’ï¼‰
       â”‚
       â–¼
Transformation (map/filter/join) æ„å»º DAG
       â”‚
       â–¼
Action (collect/count/save) è§¦å‘è®¡ç®—
       â”‚
       â–¼
DAG Scheduler æ‹†åˆ† Stage
       â”‚
       â–¼
Task Scheduler åˆ†å‘ Task åˆ° Executor
       â”‚
       â–¼
Executor æ‰§è¡Œ Taskï¼ˆè®¡ç®— + Shuffleï¼‰
       â”‚
       â–¼
ç»“æœè¿”å› Driver æˆ–è¾“å‡ºåˆ°å­˜å‚¨
```


#### æ ¸å¿ƒç‰¹ç‚¹

* **æƒ°æ€§è®¡ç®—**ï¼šTransformation ä¸ç«‹å³æ‰§è¡Œï¼Œä¼˜åŒ– DAGã€‚
* **DAG è°ƒåº¦**ï¼šæ¯” MapReduce æ›´çµæ´»é«˜æ•ˆã€‚
* **å†…å­˜è®¡ç®—**ï¼šå‡å°‘ç£ç›˜ I/Oï¼ŒåŠ é€Ÿè¿­ä»£è®¡ç®—ã€‚
* **å®¹é”™æœºåˆ¶**ï¼šRDD å¯é€šè¿‡ lineageï¼ˆè¡€ç»Ÿï¼‰é‡ç®—ä¸¢å¤±åˆ†åŒºã€‚
* **åˆ†å¸ƒå¼å¹¶è¡Œ**ï¼šStage å†…ä»»åŠ¡å¯å¹¶è¡Œæ‰§è¡Œï¼Œæé«˜é›†ç¾¤åˆ©ç”¨ç‡ã€‚



### åº”ç”¨åœºæ™¯

* æ—¥å¿—åˆ†æã€å®æ—¶ç›‘æ§ã€‚
* ETL æ•°æ®å¤„ç†ä¸æ•°æ®ä»“åº“æ„å»ºã€‚
* æ¨èç³»ç»Ÿã€å¹¿å‘Šç‚¹å‡»é¢„æµ‹ã€‚
* é‡‘èé£æ§ä¸é£é™©è®¡ç®—ã€‚
* ç¤¾äº¤ç½‘ç»œå›¾åˆ†æã€‚



### è¿›é˜¶å­¦ä¹ æ–¹å‘

* **Spark SQL ä¼˜åŒ– (Catalyst Optimizer)**
* **DataFrame vs RDD æ€§èƒ½æ¯”è¾ƒ**
* **Spark Streaming å®æ—¶æ•°æ®æµ**
* **MLlib æœºå™¨å­¦ä¹ ç®¡çº¿**
* **GraphX å›¾è®¡ç®—**
* **åœ¨é›†ç¾¤ä¸Šéƒ¨ç½² Spark (YARN/K8s)**



<br>


## å®‰è£…

### ç³»ç»Ÿè¦æ±‚
- æ”¯æŒçš„æ“ä½œç³»ç»Ÿï¼šWindows / macOS / Linux
- éœ€è¦å®‰è£…ï¼š
  - **Java 8 æˆ–ä»¥ä¸Šç‰ˆæœ¬**
  - **Python 3.7+**ï¼ˆç”¨äº PySparkï¼‰
  - **Scala 2.12+**ï¼ˆå¦‚æœä½¿ç”¨ Scalaï¼‰
  - **Hadoop å¯é€‰**ï¼ˆç”¨äºé›†ç¾¤æˆ– HDFS å­˜å‚¨ï¼‰

### æ£€æŸ¥ä¾èµ–

```bash
java -version
python --version
scala -version
```

### Windows

- æ­¥éª¤ 1ï¼šå®‰è£… Java

æ‰“å¼€å®˜ç½‘ https://www.oracle.com/java/technologies/downloads/

ä¸‹è½½å¹¶å®‰è£… Java 8 æˆ– 11 (JDK)ã€‚

å®‰è£…å®Œæˆåï¼Œåœ¨å‘½ä»¤è¡Œï¼ˆWin+R â†’ è¾“å…¥ cmdï¼‰ä¸­è¾“å…¥ï¼š
```shell
java -version
```

å¦‚æœèƒ½çœ‹åˆ°ç‰ˆæœ¬å·ï¼ˆä¾‹å¦‚ java version "1.8.0_371"ï¼‰ï¼Œè¯´æ˜æˆåŠŸã€‚

- æ­¥éª¤ 2ï¼šå®‰è£… Spark

å‰å¾€ Apache Spark å®˜ç½‘ä¸‹è½½é¡µé¢ï¼š
ğŸ”— https://spark.apache.org/downloads.html

ä¸‹è½½é…ç½®ï¼š

Choose Spark release: 3.5.0 (or latest)

Package type: Pre-built for Apache Hadoop 3

ä¸‹è½½ .tgz æ–‡ä»¶åï¼Œç”¨ 7-Zip æˆ– WinRAR è§£å‹åˆ°ä¸€ä¸ªç›®å½•ï¼ˆä¾‹å¦‚ï¼š
`C:\spark\spark-3.5.0-bin-hadoop3`ï¼‰ã€‚

- æ­¥éª¤ 3ï¼šå®‰è£… Hadoop Winutils

ä¸‹è½½ Hadoop Windows å·¥å…·åŒ…ï¼š
ğŸ”— https://github.com/steveloughran/winutils

è¿›å…¥ä¸ Spark Hadoop ç‰ˆæœ¬å¯¹åº”çš„æ–‡ä»¶å¤¹ï¼ˆå¦‚ hadoop-3.2.2ï¼‰ï¼Œä¸‹è½½æ•´ä¸ªæ–‡ä»¶å¤¹ã€‚

æ”¾åˆ°ï¼š

`C:\hadoop\bin\winutils.exe`

- æ­¥éª¤ 4ï¼šè®¾ç½®ç¯å¢ƒå˜é‡

æ‰“å¼€ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼ˆæœç´¢â€œç¼–è¾‘ç³»ç»Ÿç¯å¢ƒå˜é‡â€ï¼‰â†’â€œç¯å¢ƒå˜é‡â€ï¼š
```
å˜é‡å	å€¼
JAVA_HOME	C:\Program Files\Java\jdk-1.8.0_xxx
SPARK_HOME	C:\spark\spark-3.5.0-bin-hadoop3
HADOOP_HOME	C:\hadoop
PATH	æ·»åŠ ï¼š%SPARK_HOME%\bin å’Œ %HADOOP_HOME%\bin
```



### Linux/MacOS

å®‰è£… Javaï¼ˆJDKï¼‰
Windows / macOS / Linux é€šç”¨æ–¹å¼

ä¸‹è½½å¹¶å®‰è£… OpenJDKï¼š
```
# Ubuntu
sudo apt update
sudo apt install openjdk-11-jdk -y

# macOS (ä½¿ç”¨ Homebrew)
brew install openjdk@11
```

è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
```
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH
```

- å®‰è£… Spark

ä»å®˜ç½‘ä¸‹è½½å®‰è£…åŒ…:

è®¿é—® [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)

**æ¨èé…ç½®ï¼š**
* Spark version: 3.5.xï¼ˆæˆ–æœ€æ–°ç¨³å®šç‰ˆï¼‰
* Package type: *Pre-built for Apache Hadoop 3.3 and later*
* Download type: *Direct Download*

- ä¸‹è½½åè§£å‹ï¼š

```bash
tar -xvzf spark-3.5.0-bin-hadoop3.tgz
mv spark-3.5.0-bin-hadoop3 /usr/local/spark
```

- ç¯å¢ƒå˜é‡é…ç½®


åœ¨ `~/.bashrc` æˆ– `~/.zshrc` æœ«å°¾æ·»åŠ ï¼š

```bash
export SPARK_HOME=/usr/local/spark
export PATH=$SPARK_HOME/bin:$PATH
export PYSPARK_PYTHON=python3
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

ä¿å­˜åæ‰§è¡Œï¼š

```bash
source ~/.bashrc
```



### éªŒè¯å®‰è£…

è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
spark-shell
```

å‡ºç°ç±»ä¼¼ï¼š

```
Spark context Web UI available at http://localhost:4040
```

è¡¨ç¤ºå®‰è£…æˆåŠŸã€‚

é€€å‡º shellï¼š

```bash
:quit
```

<br>

## ä½¿ç”¨

### é…ç½®æ–‡ä»¶è¯´æ˜

é…ç½®è·¯å¾„ï¼š`$SPARK_HOME/conf`

å¸¸ç”¨é…ç½®ï¼š

* `spark-env.sh`ï¼šç¯å¢ƒå˜é‡ï¼ˆå¦‚ JAVA_HOMEã€PYSPARK_PYTHONï¼‰
* `spark-defaults.conf`ï¼šé»˜è®¤å‚æ•°ï¼ˆå¦‚ masterã€driver-memoryï¼‰
* `log4j2.properties`ï¼šæ—¥å¿—é…ç½®

ç¤ºä¾‹ï¼ˆspark-defaults.confï¼‰ï¼š

```
spark.master                     local[*]
spark.app.name                   MySparkApp
spark.driver.memory               2g
spark.executor.memory             2g
```

### è¿è¡Œ PySpark

PySpark: Python äº¤äº’å¼æ¨¡å¼

```bash
pyspark
```

ç¤ºä¾‹ï¼š

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TestApp").getOrCreate()
data = [(1, "Alice"), (2, "Bob"), (3, "Cathy")]
df = spark.createDataFrame(data, ["id", "name"])
df.show()
```

è¾“å‡ºï¼š

```
+---+-----+
| id| name|
+---+-----+
|  1|Alice|
|  2|  Bob|
|  3|Cathy|
+---+-----+
```


### æäº¤ Spark åº”ç”¨ç¨‹åº

å°† Python æ–‡ä»¶ä¿å­˜ä¸º `example.py`ï¼š

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("WordCount").getOrCreate()
data = spark.read.text("README.md")
counts = data.selectExpr("explode(split(value, ' ')) as word") \
             .groupBy("word").count().orderBy("count", ascending=False)
counts.show(10)
spark.stop()
```

ä½¿ç”¨ `spark-submit` è¿è¡Œï¼š

```bash
spark-submit example.py
```


* ä½¿ç”¨ `spark-submit` æäº¤åº”ç”¨åˆ°é›†ç¾¤ï¼š

```bash
spark-submit \
   --master yarn \
   --deploy-mode cluster \
   --class com.example.MyApp \
   myapp.jar
```


### Spark Web UI

å¯åŠ¨ä»»åŠ¡åï¼Œå¯è®¿é—®ï¼š

* **é»˜è®¤ç«¯å£**ï¼š`http://localhost:4040`
* æ˜¾ç¤ºå†…å®¹ï¼š

  * Jobã€Stageã€Executor ä¿¡æ¯
  * å­˜å‚¨ä¸å†…å­˜ä½¿ç”¨æƒ…å†µ





### Spark è¿è¡Œæ¨¡å¼

| æ¨¡å¼             | å¯åŠ¨å‘½ä»¤                                 | è¯´æ˜           |
| -------------- | ------------------------------------ | ------------ |
| **Local**      | `spark-shell --master local[*]`      | å•æœºå¤šçº¿ç¨‹æ¨¡å¼      |
| **Standalone** | `start-master.sh`, `start-worker.sh` | Spark è‡ªå¸¦é›†ç¾¤æ¨¡å¼ |
| **YARN**       | `--master yarn`                      | Hadoop é›†ç¾¤ä¸Šè¿è¡Œ |
| **Kubernetes** | `--master k8s://...`                 | åœ¨ K8s é›†ç¾¤ä¸Šè¿è¡Œ  |

---

### Standalone é›†ç¾¤æ¨¡å¼ç¤ºä¾‹

```bash
# å¯åŠ¨ master
$SPARK_HOME/sbin/start-master.sh

# å¯åŠ¨ workerï¼ˆworker èŠ‚ç‚¹éœ€é…ç½® SPARK_HOMEï¼‰
$SPARK_HOME/sbin/start-worker.sh spark://<master-ip>:7077

# æŸ¥çœ‹ UI
http://<master-ip>:8080
```

è¿è¡Œä»»åŠ¡ï¼š

```bash
spark-submit --master spark://<master-ip>:7077 example.py
```



### Spark ä¸ Jupyter Notebook é›†æˆ

å®‰è£…ä¾èµ–ï¼š

```bash
pip install findspark pyspark
```

åœ¨ Notebook é¡¶éƒ¨æ·»åŠ ï¼š

```python
import findspark
findspark.init("/usr/local/spark")

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("JupyterSpark").getOrCreate()
```

éªŒè¯ï¼š

```python
df = spark.createDataFrame([(1, "apple"), (2, "banana")], ["id", "fruit"])
df.show()
```


### æ•°æ®åˆ†æç®€ä¾‹

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import avg

spark = SparkSession.builder.appName("DataExample").getOrCreate()

data = [(1, "A", 10), (2, "B", 15), (3, "A", 20)]
df = spark.createDataFrame(data, ["id", "group", "value"])

df.groupBy("group").agg(avg("value").alias("avg_value")).show()
```

è¾“å‡ºï¼š

```
+-----+----------+
|group|avg_value |
+-----+----------+
|  A  |   15.0   |
|  B  |   15.0   |
+-----+----------+
```









## å¸¸è§é—®é¢˜ä¸è§£å†³

| é—®é¢˜                                    | åŸå›        | è§£å†³æ–¹æ¡ˆ                           |
| ------------------------------------- | -------- | ------------------------------ |
| `JAVA_HOME not set`                   | ç¯å¢ƒå˜é‡ç¼ºå¤±   | åœ¨ spark-env.sh è®¾ç½® JAVA_HOME    |
| `pyspark: command not found`          | PATH æœªé…ç½® | å°† `$SPARK_HOME/bin` æ·»åŠ åˆ° PATH   |
| `Connection refused (localhost:4040)` | UI å·²å ç”¨ç«¯å£ | ä½¿ç”¨ `--conf spark.ui.port=4050` |
| å†…å­˜ä¸è¶³é”™è¯¯                                | æ•°æ®é‡è¿‡å¤§    | å¢åŠ  driver/executor å†…å­˜          |

<br>

## è¯¾ç¨‹ä½œä¸š1

è¯¾ç¨‹ä½œä¸š1---Sparkå®‰è£…åŠä½¿ç”¨

### ä»»åŠ¡1
ä»»åŠ¡1ï¼ˆ5åˆ†ï¼‰ï¼šSparkå®‰è£…ã€‚åœ¨Linux/Windows/Macï¼ˆä¸‰è€…é‡Œé€‰ä¸€ä¸ªï¼‰ç³»ç»Ÿä¸Šå®‰è£…Sparkï¼Œä¸è¦æ±‚è¿›è¡Œåˆ†å¸ƒå¼é›†ç¾¤é…ç½®ï¼Œåªéœ€å•æœºç‰ˆè·‘é€šå³å¯ã€‚
æäº¤è¦æ±‚ï¼šå®‰è£…æˆåŠŸåï¼Œå¯åŠ¨Sparkçš„å±å¹•æˆªå±
<img src="https://raw.githubusercontent.com/YukinoshitaSherry/qycf_picbed/main/img/20251009202555122.png"  style="width:60%">
<img src="https://raw.githubusercontent.com/YukinoshitaSherry/qycf_picbed/main/img/20251009202548939.png"  style="width:60%"><br>

### ä»»åŠ¡2
ä»»åŠ¡2ï¼ˆ3åˆ†ï¼‰ï¼šSparkå®˜æ–¹åŒ…ä¸­æä¾›äº†å¤šç§è¯­è¨€ç¼–å†™çš„å¤šä¸ªexampleç¨‹åºï¼Œè¿è¡ŒSparkè‡ªå¸¦çš„wordcountç¤ºä¾‹ï¼ŒåŠŸèƒ½æ˜¯ç»Ÿè®¡ç»™å®šçš„æ–‡æœ¬æ–‡ä»¶ä¸­æ¯ä¸€ä¸ªå•è¯å‡ºç°çš„æ€»æ¬¡æ•°ã€‚åœ¨ç†Ÿæ‚‰wordcountç¤ºä¾‹ä¹‹åï¼Œå°†food_deliveryçš„Order_Dateåˆ—æå–å‡ºæ¥ï¼Œå¦å­˜ä¸ºä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ŒåŸºäºè¯¥è¾“å…¥æ–‡ä»¶è¿è¡Œwordcountç¤ºä¾‹ï¼Œç»Ÿè®¡æ¯ä¸€å¤©çš„é…é€è®¢å•æ•°é‡ã€‚
æäº¤è¦æ±‚ï¼šæäº¤ç¨‹åºè¿è¡Œè¾“å‡ºçš„å±å¹•æˆªå±ï¼ŒåŒ…æ‹¬æ¯ä¸€å¤©çš„è®¢å•æ•°é‡

<img src="https://raw.githubusercontent.com/YukinoshitaSherry/qycf_picbed/main/img/20251009202536332.png"  style="width:60%"><br>

### ä»»åŠ¡3
ä»»åŠ¡3ï¼ˆ6åˆ†ï¼‰ï¼šåŸºäºSparkç¼–å†™ä»£ç å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼šæŸ¥æ‰¾å¹´é¾„æœ€å¤§çš„é…é€äººå‘˜çš„Delivery_person_IDã€‚
æäº¤è¦æ±‚ï¼šæäº¤ç¼–å†™çš„ä»£ç ï¼Œç¨‹åºç»“æœè¿è¡Œè¾“å‡ºçš„æˆªå±ã€‚


```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder()
  .appName("FindOldestDeliveryPerson")
  .getOrCreate()

// è¯»å– CSV
val df = spark.read.option("header", "true").csv("food_delivery.csv")

// å°† Delivery_person_Age åˆ—è½¬æ¢ä¸ºæ•´æ•°ç±»å‹
val dfWithAge = df.withColumn("Delivery_person_Age", col("Delivery_person_Age").cast("int"))

// æ‰¾åˆ°æœ€å¤§å¹´é¾„
val maxAge = dfWithAge.agg(max("Delivery_person_Age")).collect()(0)(0)
println(s"Max age: $maxAge")

// æ‰¾å‡ºæ‰€æœ‰å¹´é¾„ç­‰äºæœ€å¤§å¹´é¾„çš„é…é€å‘˜ID
val oldest = dfWithAge.filter(col("Delivery_person_Age") === maxAge)
  .select("Delivery_person_ID")
  .distinct()

// æ˜¾ç¤ºç»“æœ
oldest.show()

spark.stop()

```

<br>
<img src="https://raw.githubusercontent.com/YukinoshitaSherry/qycf_picbed/main/img/20251009202517400.png"  style="width:60%"><br>



### ä»»åŠ¡4
ä»»åŠ¡4ï¼ˆ6åˆ†ï¼‰ï¼šåŸºäºSparkç¼–å†™ä»£ç å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼šç»Ÿè®¡2022å¹´3æœˆä»½çš„å¤–å–è®¢å•çš„å¹³å‡é…é€æ—¶é—´ã€‚
æäº¤è¦æ±‚ï¼šæäº¤ç¼–å†™çš„ä»£ç ï¼Œç¨‹åºç»“æœè¿è¡Œè¾“å‡ºçš„æˆªå±ã€‚

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder()
  .appName("AvgDeliveryTimeMarch2022")
  .getOrCreate()

// è¯»å– CSV
val df = spark.read.option("header","true").csv("food_delivery.csv")

// è½¬æ¢ Order_Date åˆ—ä¸ºæ—¥æœŸç±»å‹ï¼ˆå‡è®¾æ ¼å¼ä¸º dd-MM-yyyyï¼‰
val dfWithDate = df.withColumn("Order_Date", to_date(col("Order_Date"), "dd-MM-yyyy"))

// æ¸…ç† Time_taken(min) åˆ—å¹¶è½¬æ¢ä¸ºæ•´æ•°
val dfWithTime = dfWithDate.withColumn("Time_taken_min",
  regexp_replace(col("Time_taken(min)"), "[^0-9]", "").cast("int")
)

// è¿‡æ»¤ 2022 å¹´ 3 æœˆçš„æ•°æ®
val march = dfWithTime.filter(col("Order_Date") >= "2022-03-01" && col("Order_Date") <= "2022-03-31")

// è®¡ç®—å¹³å‡é…é€æ—¶é—´
val avgTimeValue = march.agg(avg("Time_taken_min")).collect()(0)(0)

// å°†ç»“æœå¼ºåˆ¶è½¬æ¢ä¸º Double
val avgTime = avgTimeValue match {
  case d: Double => d
  case n: Number => n.doubleValue()
  case _ => 0.0
}

println(f"Average delivery time in March 2022 (minutes): $avgTime%.2f")

spark.stop()


```

<br><img src="https://raw.githubusercontent.com/YukinoshitaSherry/qycf_picbed/main/img/20251009202501258.png"  style="width:60%"><br>

æ‰§è¡Œæ–¹æ³•:
å°† Scala ä»£ç ä¿å­˜ä¸º `FindOldestDeliveryPerson.scala` å’Œ `AvgDeliveryTimeMarch2022.scala`ã€‚
ä½¿ç”¨ `spark-shell` äº¤äº’å¼è¿è¡Œï¼Œæˆ–è€…ç”¨ `sbt / spark-submit` ç¼–è¯‘æäº¤ï¼š
```shell
spark-shell -i FindOldestDeliveryPerson.scala
spark-shell -i AvgDeliveryTimeMarch2022.scala
```

å¯èƒ½ä¼šæœ‰æŠ¥é”™ï¼š`java.nio.charset.MalformedInputException: Input length = 1`
è§£å†³åŠæ³•ï¼šå°†æ–‡ä»¶ä¿å­˜ä¸ºUTF-8æ ¼å¼ï¼ˆVSCodeå³ä¸‹è§’ç‚¹å‡»UTF-8ï¼‰ï¼Œæˆ–è€…æŠŠæ³¨é‡Šæ”¹æˆè‹±è¯­oråˆ é™¤ã€‚