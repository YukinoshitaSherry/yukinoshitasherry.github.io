---
title: 常见指标之pass@k, avg@k, const@k, best@k
date: 2026-01-03
categories:
    - 学AI/DS
tags:
    - LLM
    - Agent
    - Benchmark
desc: 在很多LLM的评测报告中，可能会看到这些指标，尤其是在代码生成、数学推理、程序合成等任务里。它们的侧重各不相同，但都基于一个前提设定：对同一个问题，模型允许生成 k 个不同的答案，再用不同方式来统计表现。
---

在很多LLM的评测报告中，可能会看到这些指标，尤其是在代码生成、数学推理、程序合成等任务里。
它们的侧重各不相同，但都基于一个前提设定：对同一个问题，模型允许生成 k 个不同的答案，再用不同方式来统计表现。


## 指标详解

### pass@k

【模型能不能"做到过"】 【模型有没有解决这个问题的潜力？】

**定义**：pass@k 的定义非常直接：对每个问题，模型生成 k 个答案，只要其中有至少一个答案是正确的，这个问题就算"通过"。

**数学表达**：
$$
\text{pass@k} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}[\exists j \in \{1,2,\ldots,k\}, \text{answer}_{i,j} \text{ is correct}]
$$

其中 $n$ 是问题总数，$\mathbb{1}[\cdot]$ 是指示函数。

**衡量内容**：
- 如果我不断采样，模型有没有能力在某一次把题做对？
- 它关注的是**模型的能力上限**，而不是稳定性。

**适用场景**：
- 适合代码生成等允许多次尝试的任务
    - 代码生成任务中非常常见，比如 HumanEval、MBPP 等
    - 代码任务天然允许"多试几次"
- 适合评估模型的**潜力**而非实际可靠性
    - 不适合评估实际部署中的可靠性



**关键点**：
- pass@k 高不代表模型"更聪明"
- 只说明模型在多次尝试后有可能解决这个问题
- 适合评估模型的**能力上限**



**局限性**：
- **对采样温度和多样性非常敏感**：一个模型即使31次都错，只要第32次对了，pass@32 也是1
- **不反映模型在实际使用中的可靠性**：pass@k 高不代表模型"更聪明"，只说明模型有解决这个问题的潜力
- 可能高估模型能力

**数据类型**：**离散型**（0或1，每个问题要么通过要么不通过）





**计算示例**：

```text
问题1：生成3个答案，答案1错误，答案2正确，答案3错误 → pass@3 = 1
问题2：生成3个答案，全部错误 → pass@3 = 0
总体：pass@3 = (1 + 0) / 2 = 0.5
```

<br>

### avg@k

【模型通常有多靠谱】

**定义**：avg@k 是对 k 个样本逐一判断对错，然后取平均。

**数学表达**：
$$
\text{avg@k} = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{k} \sum_{j=1}^{k} \mathbb{1}[\text{answer}_{i,j} \text{ is correct}]
$$

**本质含义**：
- k 个答案里，有多少比例是正确的
- 本质上是"随机抽一个答案，答对的概率是多少"

**回答的问题**：
- 模型在正常采样条件下，平均有多大概率给出正确答案？

**相比 pass@k 的特点**：
- **更严格**：模型必须经常答对，而不是偶尔蒙中
- **对"不稳定但偶尔成功"的模型非常不友好**：即使模型偶尔能答对，如果大部分时候都错，avg@k 会很低

**适用场景**：
- 更常见于数学、推理等需要稳定输出的任务
- 用来衡量模型的**稳定性和校准程度**
- 适合评估模型的**实际使用可靠性**
    - 适合评估实际部署场景



**关键点**：
- avg@k 低，可能说明模型推理不稳定
- 反映模型的**平均表现**和**实际可用性**
- 适合评估模型的**稳定性**

**局限性**：
- 可能低估模型能力上限
- 对不稳定但偶尔成功的模型不友好


**使用建议**：
- 适合数学、推理


**数据类型**：**连续型**（取值在 [0, 1] 区间内，可以是任意小数）

**计算示例**：

```text
问题1：生成3个答案，答案1错误，答案2正确，答案3错误 → avg@3 = 1/3 ≈ 0.333
问题2：生成3个答案，全部错误 → avg@3 = 0
总体：avg@3 = (1/3 + 0) / 2 ≈ 0.167
```

<br>

### cons@k

【模型"最相信"的答案对不对】【模型在多次思考后，最可能给出的答案是不是对的？】

**定义**：cons@k (consensus@k) 引入了一个投票或一致性机制。

**计算步骤**：
1. 对同一个问题生成 k 个答案
2. 提取每个答案的最终结果
3. 选出现频率最高的那个作为模型的"共识答案"
4. 判断这个共识答案是否正确

**数学表达**：
$$
\text{cons@k} = \frac{1}{n} \sum\_{i=1}^{n} \mathbb{1}[\text{mode}(\{\text{answer}\_{i,1}, \ldots, \text{answer}\_{i,k}\}) \text{ is correct}]
$$

其中 $\text{mode}(\cdot)$ 表示众数（出现频率最高的值）。

**衡量内容**：
- 在多次思考后，模型最稳定、最一致的结论是不是对的？

**适用场景**：
- 在链式推理、数学证明、符号推理任务中非常常见
    - 适合需要模型"深思熟虑"的场景
- 通常被称为 **self-consistency** 评测
- 适合评估模型的**内部概率结构一致性**


**关键点**：
- cons@k 高通常意味着模型内部概率结构更一致
- 反映模型的**置信度校准**：模型最相信的答案是否真的正确
- 适合评估模型的**一致性**


**局限性**：
- 需要答案可以提取和比较
- 平局处理可能影响结果
- 不适合答案形式差异很大的任务

**直觉理解**：
- 如果模型对某个正确解路径有更强的概率集中，那多次采样后，这个答案更容易成为"多数派"
- 体现了模型的**置信度校准**：模型最相信的答案是否真的正确

**数据类型**：**离散型**（0或1，每个问题的共识答案要么对要么错）

**计算示例**：

```text
问题1：生成5个答案，答案1="42"，答案2="42"，答案3="43"，答案4="42"，答案5="43"
       共识答案="42"（出现3次），如果"42"正确 → cons@5 = 1
问题2：生成5个答案，答案1="A"，答案2="B"，答案3="C"，答案4="A"，答案5="B"
       共识答案="A"或"B"（各出现2次，需处理平局），如果都不正确 → cons@5 = 0
```

**平局处理**：
- 当多个答案出现频率相同时，通常采用随机选择或选择第一个
- 有些实现会要求严格多数（> k/2）才形成共识

<br>

### best@k

【模型在最优选择下的表现】

**定义**：best@k 是选择 k 个答案中**最好的那个**作为最终答案，然后判断是否正确。

**与 pass@k 的区别**：
- pass@k：只要有一个对就算通过（存在性判断）
- best@k：选择最好的答案，判断它是否正确（最优性判断）

**适用场景**：
- 当答案有质量评分时（如代码有功能正确性、代码质量、可读性等多维度评分）
- 评估模型在**最优输出选择**下的表现
- 常用于需要人工评估或有多维度评分的任务


**关键点**：
- 评估模型在**最优输出选择**下的表现
- 需要定义"最好"的标准
- 适合有多维度评分的任务

**数据类型**：**离散型**（0或1）

**计算示例**：

```text
问题1：生成3个答案
       - 答案1：功能正确，代码质量7分
       - 答案2：功能正确，代码质量9分（最好）
       - 答案3：功能错误
       选择答案2，如果答案2正确 → best@3 = 1
```

**注意**：best@k 需要定义"最好"的标准，可能是：
- 功能正确性优先
- 综合评分最高
- 人工评估最优
- 多目标优化的帕累托最优

<br>

## 指标关系

### 大小关系

在实际实验中，大多数模型都满足下面的关系：

$$
\text{avg@k} \leq \text{cons@k} \leq \text{pass@k}
$$

**best@k 的位置**：

best@k 与上述三个指标的关系**取决于"最好"的定义**，但通常满足：

$$
\text{best@k} \leq \text{pass@k}
$$

这是因为：如果最好的答案都正确，那么必然存在至少一个正确答案，所以 best@k ≤ pass@k。

**best@k 与其他指标的关系**（取决于评分标准）：

- **如果"最好"=功能正确性优先**：best@k 通常 ≤ pass@k，但不一定与其他指标有固定关系
- **如果"最好"=综合评分最高**：best@k 可能高于或低于 avg@k、cons@k，取决于评分标准的偏向
- **best@k 与 avg@k、cons@k 的关系不固定**：因为"最好"的定义独立于正确性统计

#### 理解分析

- **avg@k 是下界**：代表一次随机采样的表现，反映模型的平均可靠性
- **pass@k 是上界**：代表"试够多次总能成功"，反映模型的能力上限
- **cons@k 介于两者之间**：体现稳定性和一致性，反映模型内部概率结构的集中程度
- **best@k 独立位置**：取决于评分标准，通常 ≤ pass@k，但与其他指标关系不固定


1. **avg@k ≤ pass@k**：
   - avg@k 是 k 个答案的平均正确率
   - pass@k 只要有一个对就算1，所以 pass@k ≥ avg@k
   - 当且仅当所有答案要么全对要么全错时，两者相等

2. **cons@k ≤ pass@k**：
   - cons@k 要求共识答案正确
   - pass@k 只要任意一个答案正确即可
   - 如果共识答案正确，必然存在至少一个正确答案，所以 cons@k ≤ pass@k

3. **avg@k ≤ cons@k**（通常成立，但不总是）：
   - 当模型对正确答案有较强的概率集中时，共识答案更可能是正确的
   - 但这不是严格的数学关系，取决于模型的行为模式

4. **best@k ≤ pass@k**（通常成立）：
   - 如果最好的答案都正确，必然存在至少一个正确答案
   - 但反过来说，如果存在正确答案，"最好"的定义可能导致选择错误的答案（如果评分标准偏向非正确性因素）

#### 特殊情况

- **理想情况**：当模型非常稳定且正确时，avg@k ≈ cons@k ≈ pass@k ≈ 1，best@k ≈ 1（如果评分标准偏向正确性）
- **不稳定但偶尔成功**：avg@k 很低，但 pass@k 可能较高，best@k 取决于评分标准
- **稳定但错误**：avg@k 和 cons@k 都低，pass@k 也低，best@k 也低
- **best@k 的特殊情况**：如果评分标准偏向代码质量而非正确性，best@k 可能高于或低于其他指标

<br>

### 能否互推？

#### 严格的数学关系（可以互推范围）

**avg@k、cons@k、pass@k 之间的关系**：

1. **已知 avg@k，可以推导 pass@k 的下界**：
   - 当 avg@k = p 时，pass@k ≥ p（当所有答案要么全对要么全错时取等）
   - 但无法确定 pass@k 的上界（理论上可以接近1）

2. **已知 pass@k，可以推导 avg@k 的上界**：
   - 当 pass@k = p 时，avg@k ≤ p（同样当所有答案要么全对要么全错时取等）
   - 无法确定 avg@k 的下界

3. **cons@k 与 avg@k、pass@k 的关系**：
   - 通常 avg@k ≤ cons@k ≤ pass@k，但这不是严格的，取决于模型行为
   - **无法从 avg@k 或 pass@k 精确推导 cons@k**

**结论**：avg@k、cons@k、pass@k 之间**不能完全互推**，只能推导出范围或界限。

#### best@k 与其他指标的关系（不能互推）

**best@k 无法从其他指标推导**，原因：

1. **"最好"的定义独立**：
   - best@k 依赖于评分标准（功能正确性、代码质量、可读性等）
   - 这些标准可能与正确性统计无关

2. **可能出现的情况**：
   - pass@k 很高，但 best@k 很低：如果评分标准偏向非正确性因素，最好的答案可能是错误的
   - avg@k 很低，但 best@k 很高：如果模型偶尔生成高质量但错误的答案
   - 无法从其他指标预测 best@k 的值

**结论**：best@k **不能从其他指标推导**，必须独立计算。

<br>

### 横向比较


**不能直接横向比较不同指标的数值大小**，原因：

1. **衡量维度不同**：
   - pass@k：能力上限（存在性）
   - avg@k：平均可靠性（连续性）
   - cons@k：一致性（稳定性）
   - best@k：最优输出（质量导向）

2. **数值意义不同**：
   - pass@k = 0.8 表示80%的问题至少有一个正确答案
   - avg@k = 0.8 表示80%的答案平均正确
   - 这两个0.8的含义完全不同

3. **示例说明为什么不能直接比较**：

   ```text
   模型A：pass@10 = 0.9, avg@10 = 0.1
   模型B：pass@10 = 0.5, avg@10 = 0.5
   ```
   - 不能说模型A "更好"：模型A不稳定，只是偶尔成功
   - 不能说模型B "更好"：模型B能力上限较低
   - 需要根据任务需求选择：代码生成关注 pass@k，实际部署关注 avg@k

正确的比较方式：同类指标比较、多指标综合评估。




<br>

## 数据类型

### 连续型 vs 离散型

| 指标 | 数据类型 | 取值范围 | 说明 |
| ---- | -------- | -------- | ---- |
| **pass@k** | 离散型 | {0, 1} | 每个问题要么通过(1)要么不通过(0) |
| **avg@k** | 连续型 | [0, 1] | 可以是任意小数，表示平均正确率 |
| **cons@k** | 离散型 | {0, 1} | 每个问题的共识答案要么对(1)要么错(0) |
| **best@k** | 离散型 | {0, 1} | 每个问题的最优答案要么对(1)要么错(0) |

### 判断方法

1. **观察取值范围**：
   - 如果只能取有限个特定值（如0或1），则为离散型
   - 如果可以在区间内取任意值，则为连续型

2. **考虑计算方式**：
   - 计数/存在性判断 → 离散型
   - 平均值/比例 → 连续型

3. **实际意义**：
   - 离散型：二元判断（对/错）
   - 连续型：概率或比例

<br>

### 判断数据异常

发现了就说明，该debug了！

### 指标值异常

**常见问题**：
- **pass@k 异常高但 avg@k 很低**：说明模型不稳定，只是偶尔蒙对
  - 例如：pass@100 = 0.9 但 avg@100 = 0.05
  - 这表示模型在100次尝试中只有5%正确，但至少有一次正确的概率是90%
  
- **cons@k 与 avg@k 差距过大**：说明模型内部概率结构不一致
  - 例如：avg@k = 0.8 但 cons@k = 0.3
  - 这表示虽然平均正确率高，但模型最相信的答案往往是错的

- **违反数学关系**：如果 avg@k > pass@k，数据肯定有问题
  - 这违反了基本数学关系，可能是计算错误

### 采样相关问题

**温度设置不当**：
- 温度过高：生成答案过于随机，avg@k 和 cons@k 都会很低
- 温度过低：生成答案过于确定，可能无法充分探索解空间

**采样数量不足**：
- k 值太小：pass@k 可能低估模型能力
- k 值太大：计算成本高，且可能引入噪声

**建议**：
- 代码生成任务：k = 1, 10, 100 是常见选择
- 数学推理任务：k = 1, 5, 10 是常见选择

### 评估标准问题

**判断标准不一致**：
- 不同问题使用不同的判断标准
- 部分问题判断过于宽松或严格

**答案提取错误**：
- 对于 cons@k，如果答案提取方式不一致，会导致错误的共识
- 例如：代码生成中，有些提取最终输出，有些提取中间结果

### 统计显著性

**样本量不足**：
- 问题数量 n 太小，指标值不稳定
- 建议：至少 n ≥ 100，理想情况下 n ≥ 1000

**置信区间缺失**：
- 只报告点估计，不报告置信区间
- 无法判断结果的可靠性

**建议报告格式**：

```text
pass@10 = 0.65 ± 0.03 (95% CI)
avg@10 = 0.42 ± 0.02 (95% CI)
cons@10 = 0.58 ± 0.03 (95% CI)
```

### 数据集问题

**数据泄露**：
- 测试集出现在训练集中
- 导致指标虚高

**数据质量问题**：
- 测试问题本身有歧义或错误
- 标准答案不正确

**分布偏移**：
- 测试集分布与训练集差异过大
- 指标不能反映真实性能

### 实现错误

**常见错误**：
- 计算 pass@k 时，对每个问题只生成 k 个答案，但计算时用了不同的 k
- 计算 cons@k 时，平局处理不当
- 答案正确性判断逻辑有误

**验证方法**：
- 用简单例子手动计算验证
- 检查代码实现逻辑
- 对比不同实现的结果



<br>

## 计算公式与实现细节

### pass@k 的无偏估计

在实际实现中，pass@k 通常使用无偏估计：

$$
\text{pass@k} = \frac{1}{n} \sum_{i=1}^{n} \left(1 - \frac{\binom{n_i - c_i}{k}}{\binom{n_i}{k}}\right)
$$

其中：
- $n_i$ 是问题 $i$ 生成的答案总数（通常 $n_i \geq k$）
- $c_i$ 是问题 $i$ 中正确答案的数量
- $\binom{n}{k}$ 是组合数

**简化版本**（当 $n_i = k$ 时）：
$$
\text{pass@k} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}[\exists j, \text{answer}_{i,j} \text{ is correct}]
$$

### avg@k 的计算

$$
\text{avg@k} = \frac{1}{n} \sum_{i=1}^{n} \frac{c_i}{k}
$$

其中 $c_i$ 是问题 $i$ 的 k 个答案中正确答案的数量。

### cons@k 的平局处理

当多个答案出现频率相同时，常见处理方式：

1. **随机选择**：从频率最高的答案中随机选一个
2. **选择第一个**：选择第一个出现的频率最高的答案
3. **要求严格多数**：只有当某个答案出现次数 > k/2 时才形成共识
4. **加权选择**：如果答案有置信度，选择置信度最高的

### best@k 的计算

best@k 的计算依赖于评分函数 $S(\text{answer})$，该函数评估答案的质量：

$$
\text{best@k} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}[\text{correct}(\arg\max_{j \in \{1,\ldots,k\}} S(\text{answer}_{i,j}))]
$$

其中：
- $S(\text{answer}_{i,j})$ 是问题 $i$ 的第 $j$ 个答案的评分
- $\arg\max$ 选择评分最高的答案
- $\text{correct}(\cdot)$ 判断该答案是否正确
- $\mathbb{1}[\cdot]$ 是指示函数

**单维度评分**（如只考虑正确性）：
$$
\text{best@k} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}[\text{best\_answer}_i \text{ is correct}]
$$

其中 $\text{best\_answer}_i$ 是问题 $i$ 的 k 个答案中评分最高的。

**多维度评分**（如代码质量、可读性、正确性）：
$$
S(\text{answer}) = w_1 \cdot S_{\text{correctness}}(\text{answer}) + w_2 \cdot S_{\text{quality}}(\text{answer}) + w_3 \cdot S_{\text{readability}}(\text{answer})
$$

其中 $w_1, w_2, w_3$ 是权重，$\sum w_i = 1$。

**实现细节**：

1. **评分标准定义**：
   - 功能正确性：代码是否能通过测试用例
   - 代码质量：代码风格、复杂度、可维护性
   - 可读性：代码清晰度、注释完整性
   - 效率：时间复杂度、空间复杂度

2. **选择策略**：
   - **优先级策略**：先按功能正确性，再按质量评分
   - **加权平均**：多维度评分加权求和
   - **帕累托最优**：选择非支配解

3. **正确性判断**：
   - 对于 best@k，需要判断**被选为最好的答案**是否正确
   - 如果评分标准偏向非正确性因素，可能选择错误的答案作为"最好"

### 置信区间计算

对于二项分布类型的指标（pass@k, cons@k），可以使用：

**Wald 区间**（适用于大样本）：
$$
\hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
$$

**Wilson 区间**（更准确，适用于小样本）：
$$
\frac{\hat{p} + \frac{z_{\alpha/2}^2}{2n} \pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n} + \frac{z_{\alpha/2}^2}{4n^2}}}{1 + \frac{z_{\alpha/2}^2}{n}}
$$

对于连续型指标（avg@k），可以使用 t 分布区间。

<br>

## 常见基准数据集

### 代码生成基准

#### HumanEval

- **规模**：164 个手写编程问题
- **语言**：Python
- **难度**：从简单到中等
- **来源**：OpenAI (2021)
- **主要指标**：pass@k (k = 1, 10, 100)
- **评估方式**：单元测试（每个问题有多个测试用例）
- **特点**：
  - 每个问题包含函数签名、文档字符串和测试用例
  - 评估代码的功能正确性
  - 是代码生成任务的标准基准之一
- **适用指标**：pass@k（最常见）、avg@k、best@k

#### MBPP (Mostly Basic Python Problems)

- **规模**：974 个 Python 编程问题
- **难度**：基础到中等
- **来源**：Austin et al. (2021)
- **主要指标**：pass@k
- **评估方式**：单元测试
- **特点**：
  - 比 HumanEval 更大规模
  - 涵盖基本的 Python 编程概念
  - 包含函数实现、列表操作、字符串处理等
- **适用指标**：pass@k、avg@k

#### APPS (Automated Programming Progress Standard)

- **规模**：10,000 个编程竞赛问题
- **难度**：从简单到困难（入门、中级、竞赛级）
- **来源**：Hendrycks et al. (2021)
- **主要指标**：pass@k, test@k
- **评估方式**：测试用例（每个问题有多个测试用例）
- **特点**：
  - 来自在线编程竞赛平台（如 Codeforces）
  - 包含问题描述、输入输出格式、测试用例
  - 难度分级明确
- **适用指标**：pass@k、avg@k、best@k（如果有质量评分）

#### CodeXGLUE

- **规模**：多个子任务，包含 14 个数据集
- **任务类型**：代码理解、代码生成、代码检索、代码补全等
- **来源**：Microsoft Research Asia (2021)
- **主要指标**：根据子任务不同
- **评估方式**：多种（BLEU、ROUGE、精确匹配、测试用例等）
- **特点**：
  - 涵盖代码智能的多个维度
  - 包含多种编程语言（Python、Java、C#、JavaScript 等）
  - 综合评估平台
- **适用指标**：pass@k、avg@k、best@k（根据子任务）

#### DS-1000

- **规模**：1,000 个数据科学问题
- **领域**：数据科学（NumPy、Pandas、Scikit-learn 等）
- **来源**：Lai et al. (2022)
- **主要指标**：pass@k
- **评估方式**：代码执行和结果验证
- **特点**：
  - 涵盖 7 个常用数据科学库
  - 真实世界的数据科学任务
  - 需要理解数据操作和机器学习
- **适用指标**：pass@k、avg@k、best@k

#### CoderEval

- **规模**：230 个 Python 问题 + 230 个 Java 问题
- **特点**：来自真实开源项目
- **来源**：Yu et al. (2023)
- **主要指标**：pass@k
- **评估方式**：单元测试
- **特点**：
  - 任务来自真实的代码库
  - 评估实际场景中的代码生成能力
  - 包含上下文信息（类、方法等）
- **适用指标**：pass@k、best@k（代码质量评估）

#### CodeContests

- **规模**：约 13,000 个编程竞赛问题
- **来源**：Li et al. (2022)
- **主要指标**：pass@k
- **评估方式**：测试用例
- **特点**：
  - 来自 Codeforces、AtCoder 等平台
  - 难度范围广
  - 包含多种问题类型（算法、数据结构等）
- **适用指标**：pass@k、avg@k

### 数学推理基准

#### GSM8K (Grade School Math 8K)

- **规模**：8,500 个小学数学文字题
- **难度**：小学水平（多步推理）
- **来源**：Cobbe et al. (2021)
- **主要指标**：accuracy（类似 avg@k）
- **评估方式**：数值答案匹配（严格匹配和宽松匹配）
- **特点**：
  - 需要多步算术推理
  - 问题用自然语言描述
  - 答案通常是单个数字
- **适用指标**：avg@k（最常见）、cons@k、pass@k

#### MATH

- **规模**：12,500 个高中数学竞赛问题
- **难度**：高中数学竞赛水平
- **来源**：Hendrycks et al. (2021)
- **主要指标**：accuracy, pass@k
- **评估方式**：最终答案匹配（支持 LaTeX 格式）
- **特点**：
  - 涵盖代数、几何、数论、概率等多个领域
  - 需要复杂的数学推理
  - 包含完整的解题过程
- **适用指标**：avg@k、cons@k、pass@k

#### AIME (American Invitational Mathematics Examination)

- **规模**：约 1,500 个问题
- **难度**：高中数学竞赛（AMC/AIME 级别）
- **来源**：美国数学竞赛
- **主要指标**：pass@k, cons@k
- **评估方式**：数值答案匹配（0-999 的整数）
- **特点**：
  - 需要深入的数学推理
  - 答案通常是整数
  - 适合评估模型的数学推理能力
- **适用指标**：cons@k、pass@k、avg@k

#### PRM800K (Proof Writer)

- **规模**：800,000+ 个数学证明步骤
- **任务类型**：数学证明生成
- **来源**：Azerbayev et al. (2023)
- **主要指标**：证明成功率、证明步骤正确率
- **评估方式**：形式化验证
- **特点**：
  - 需要严格的逻辑推理
  - 评估证明的完整性和正确性
  - 适合评估模型的形式化推理能力
- **适用指标**：cons@k（证明一致性）、avg@k

#### NumGLUE

- **规模**：8 个数据集，涵盖数学推理的多个方面
- **任务类型**：数学问题解答、数学证明、几何推理等
- **来源**：Mishra et al. (2022)
- **主要指标**：根据数据集不同
- **评估方式**：答案匹配、证明验证等
- **特点**：
  - 涵盖数学推理的多个维度
  - 统一评估框架
- **适用指标**：avg@k、cons@k、pass@k

### 符号推理基准

#### LogiQA

- **规模**：约 8,700 个逻辑推理问题
- **任务类型**：逻辑推理（选择题）
- **来源**：Liu et al. (2020)
- **主要指标**：cons@k, accuracy
- **评估方式**：选择题答案匹配
- **特点**：
  - 需要逻辑推理能力
  - 每个问题有 4 个选项
  - 涵盖多种逻辑推理类型
- **适用指标**：cons@k（最常见）、avg@k

#### FOLIO (First-Order Logic Inference)

- **规模**：1,430 个一阶逻辑推理问题
- **任务类型**：一阶逻辑推理
- **来源**：Han et al. (2022)
- **主要指标**：cons@k
- **评估方式**：逻辑表达式匹配（True/False）
- **特点**：
  - 需要理解一阶逻辑
  - 评估形式化推理能力
  - 答案通常为 True 或 False
- **适用指标**：cons@k（最常见）、avg@k

#### ReClor

- **规模**：约 6,000 个逻辑推理问题
- **任务类型**：逻辑推理（选择题）
- **来源**：Yu et al. (2020)
- **主要指标**：accuracy, cons@k
- **评估方式**：选择题答案匹配
- **特点**：
  - 需要复杂的逻辑推理
  - 来自标准化考试（LSAT、GMAT 等）
  - 包含多种推理类型
- **适用指标**：cons@k、avg@k

#### AR-LSAT

- **规模**：逻辑推理问题
- **任务类型**：逻辑推理（基于 LSAT）
- **来源**：基于 LSAT 考试
- **主要指标**：cons@k
- **评估方式**：选择题答案匹配
- **特点**：
  - 来自法律入学考试（LSAT）
  - 需要复杂的逻辑分析
- **适用指标**：cons@k、avg@k

### 常识推理基准

#### HellaSwag

- **规模**：约 70,000 个常识推理问题
- **任务类型**：常识推理（句子补全）
- **来源**：Zellers et al. (2019)
- **主要指标**：accuracy
- **评估方式**：多项选择题
- **适用指标**：cons@k、avg@k


#### PIQA (Physical Interaction QA)

- **规模**：约 16,000 个物理常识问题
- **任务类型**：物理常识推理（选择题）
- **来源**：Bisk et al. (2020)
- **主要指标**：accuracy
- **评估方式**：选择题答案匹配
- **适用指标**：cons@k、avg@k

<br>

## 扩展概念

### 相关概念

#### Self-Consistency

- **提出者**：Wang et al. (2022)
- **核心思想**：生成多个推理路径，选择出现频率最高的答案
- **本质**：cons@k 方法的应用
- **适用场景**：数学推理、链式推理任务
- **优势**：提高推理任务的准确性
- **局限性**：需要答案可以提取和比较，对计算资源要求高
- **实现方式**：
  - 对同一问题生成多个推理路径
  - 提取每个路径的最终答案
  - 选择出现频率最高的答案
- **与指标的关系**：Self-Consistency 本质上就是 cons@k 的实践应用

#### Majority Voting

- **定义**：多模型或多次采样中，选择出现频率最高的答案
- **与 cons@k 的关系**：cons@k 可以看作 majority voting 在单个模型多次采样中的应用
- **应用场景**：模型集成、多模型协作
- **变体**：
  - **简单多数投票**：选择出现频率最高的答案
  - **加权投票**：根据模型置信度加权
  - **软投票**：概率平均而非硬选择
- **优势**：利用多个模型或多次采样的优势
- **局限性**：需要多个模型或多次采样，成本高

#### Calibration（置信度校准）

- **定义**：模型输出的置信度与真实正确率的一致性
- **与指标的关系**：
  - cons@k 高通常意味着模型置信度校准好
  - avg@k 可以反映模型的平均置信度
- **评估方法**：
  - **Expected Calibration Error (ECE)**：预期校准误差
  - **Brier Score**：Brier 分数
  - **置信度-正确率曲线**：可视化校准效果
- **重要性**：高置信度的答案应该更可能正确
- **应用**：在 best@k 中，可以使用置信度作为评分标准

<br>

### 增益方法

#### Temperature Sampling（温度采样）

- **定义**：控制生成分布的温度参数
- **对指标的影响**：
  - **温度高（T > 1）**：
    - pass@k 可能提高（探索更多解空间）
    - avg@k 可能降低（生成更随机）
    - cons@k 可能降低（答案更分散）
  - **温度低（T < 1）**：
    - pass@k 可能降低（探索不足）
    - avg@k 可能提高（更确定）
    - cons@k 可能提高（答案更集中）
- **选择建议**：
  - **代码生成**：通常 T = 0.2-0.8（探索与确定性的平衡）
  - **数学推理**：通常 T = 0.3-0.7（需要一定的探索）
  - **符号推理**：通常 T = 0.1-0.5（需要更高的确定性）
- **注意事项**：温度设置会影响所有指标，需要报告并保持一致

#### Chain-of-Thought (CoT)

- **定义**：逐步推理的方法，生成中间推理步骤
- **与指标的关系**：
  - CoT 通常提高 cons@k（更一致的推理过程）
  - CoT 可能提高 avg@k（更稳定的输出）
- **变体**：
  - **Few-shot CoT**：提供示例展示推理过程
  - **Zero-shot CoT**：直接要求模型展示推理过程
  - **Self-Consistency CoT**：结合 CoT 和 Self-Consistency
- **应用**：特别适合数学推理和符号推理任务

<br>

### 改进指标

#### Adaptive k（自适应 k）

- **思想**：根据问题难度动态调整 k 值
- **实现方式**：
  - 简单问题：k 值较小（如 k = 1-5）
  - 困难问题：k 值较大（如 k = 20-100）
- **优势**：平衡计算成本和准确性
- **挑战**：如何判断问题难度
- **可能方法**：
  - 使用模型置信度
  - 使用初步评估结果
  - 使用问题特征（长度、复杂度等）

#### Weighted Voting（加权投票）

- **思想**：在 cons@k 中使用加权投票而非简单多数
- **权重来源**：
  - 模型置信度
  - 答案质量评分
  - 历史正确率
- **优势**：考虑答案的可靠性，而不仅仅是频率
- **应用**：best@k 中可以使用加权评分
- **实现**：
  - 对每个答案计算权重
  - 加权求和而非简单计数
  - 选择权重最高的答案

#### Quality-Aware Selection（质量感知选择）

- **思想**：在 best@k 中使用多维度质量评估
- **质量维度**：
  - 功能正确性
  - 代码质量（可读性、效率、风格）
  - 安全性
  - 可维护性
- **实现方式**：
  - **多目标优化**：同时优化多个目标
  - **帕累托最优**：选择非支配解
  - **加权评分**：多维度评分加权求和
- **应用**：代码生成任务中的 best@k

#### Iterative Refinement（迭代优化）

- **思想**：基于前 k 个答案的反馈，生成更好的答案
- **应用场景**：
  - 代码生成：根据测试用例反馈改进
  - 数学推理：根据中间步骤验证改进
- **与指标的关系**：可能提高 pass@k 和 best@k
- **实现方式**：
  - 生成初始答案
  - 根据反馈改进
  - 迭代直到满意或达到最大迭代次数

#### Confidence-Based Selection（基于置信度的选择）

- **思想**：选择模型最置信的答案，而非随机选择
- **应用**：类似 best@k，但使用置信度而非质量评分
- **优势**：不需要外部评分标准
- **局限性**：依赖模型置信度校准
- **与指标的关系**：如果置信度校准好，可以提高 avg@k


<br>


### 注意事项

#### 计算成本

- **生成成本**：生成 k 个答案需要 k 倍的推理成本
  - 对于大型模型，k=100 可能需要大量 GPU 时间
  - 需要考虑时间-准确性的权衡
- **评估成本**：判断答案正确性也可能有成本
  - **代码执行**：运行测试用例需要时间
  - **人工评估**：成本更高
  - **模型评估**：使用 LLM-as-judge 也有成本
- **优化策略**：
  - 使用较小的 k 值（如果足够）
  - 并行生成和评估
  - 使用更快的评估方法

#### 公平比较

- **k 值标准化**：不同论文中的 k 值可能不同
  - 代码生成：常见 k = 1, 10, 100
  - 数学推理：常见 k = 1, 5, 10
  - 需要报告多个 k 值的结果
- **温度设置**：不同温度设置会影响结果
  - 需要报告使用的温度
  - 理想情况下应该使用相同温度
- **随机种子**：不同随机种子会产生不同结果
  - 应该固定随机种子
  - 或者报告多次运行的平均值
- **模型版本**：不同模型版本可能有差异
  - 应该报告具体的模型版本
  - 使用相同版本的模型进行比较

#### 可复现性

- **必需报告的超参数**：
  - 随机种子
  - 采样温度
  - top-p（如果使用）
  - k 值
  - 模型版本
- **环境信息**：
  - Python 版本
  - 库版本
  - 硬件信息（GPU 型号、数量）
- **评估代码**：应该公开评估代码，便于复现
- **数据信息**：报告使用的数据集版本和预处理方式

#### 统计显著性

- **样本量要求**：
  - 至少 n ≥ 100 个问题
  - 理想情况下 n ≥ 1000
- **置信区间**：
  - 应该报告置信区间
  - 使用 Wilson 区间（更准确）
- **统计检验**：
  - 比较不同模型时，应该进行统计检验
  - 使用 t-test 或 Wilcoxon 检验
- **多次运行**：建议多次运行取平均值，减少随机性

#### 指标选择

- **根据任务选择**：
  - **代码生成**：主要关注 pass@k
  - **数学推理**：主要关注 avg@k
  - **符号推理**：主要关注 cons@k
  - **质量评估**：关注 best@k
- **多指标报告**：
  - 应该同时报告多个指标
  - 全面评估模型性能
- **理解局限性**：
  - 每个指标都有局限性
  - 需要理解指标的含义和适用场景
  - 不要过度解读单个指标

<br>
