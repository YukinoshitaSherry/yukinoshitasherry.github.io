---
title: 信息论基础
date: 2023-10-20
categories:
  - 学AI/DS
tags:
  - ML
desc: 各种熵Entropy的概念与计算公式汇总。
---

- 参考：<a href="https://www.showmeai.tech/article-detail/164">`showmeai.tech`</a>

### 发展脉络

信息论的发展可以追溯到19世纪中叶，经历了以下几个重要阶段：

1. 热力学阶段（1854年）
   - 克劳修斯首次提出"熵"的概念
   - 热力学第二定律的提出
   - 奠定了熵的物理基础

2. 统计力学阶段（1877年）
   - 玻尔兹曼将熵与微观状态数联系起来
   - 建立了熵的统计解释
   - 公式：$S = k \ln W$

3. 信息论阶段（1948年）
   - 香农发表《通信的数学理论》
   - 将熵引入信息论
   - 建立了现代信息论的基础

### 熵
**Entropy**

#### 概念


熵是描述系统混乱程度或不确定性的度量。在信息论中，熵表示随机变量的不确定程度，即信息量的大小。

#### 物理意义


1. 热力学意义
   - 描述系统的混乱程度
   - 表征能量分布的均匀程度
   - 反映系统的可逆性

2. 信息论意义
   - 表示信息的不确定性
   - 度量信息的平均信息量
   - 反映信息的可压缩性

#### 数学定义


对于随机变量 $X$，其可能的取值为 $x_1, x_2, ..., x_n$，概率分布为 $P(X=x_i)=p_i$，则随机变量 $X$ 的熵定义为：

$$H(X) = -\sum_{i=1}^n p_i \log p_i$$

#### 性质

1. 非负性：$H(X) \geq 0$
2. 对称性：$H(X_1,X_2,...,X_n) = H(X_{\pi(1)},X_{\pi(2)},...,X_{\pi(n)})$
3. 可加性：$H(X,Y) = H(X) + H(Y|X)$
4. 极值性：$H(X) \leq \log |X|$

<br>

### 联合熵
**Joint Entropy**

#### 概念

联合熵用于描述多个随机变量共同的不确定性。

#### 数学定义

对于分布为 $P(X,Y)$ 的一对随机变量 $(X,Y)$，其联合熵定义为：

$$H(X,Y) = -\sum_{x \in X} \sum_{y \in Y} P(x,y) \log P(x,y)$$



### 条件熵
**Conditional Entropy**

#### 概念

条件熵描述在已知一个随机变量的条件下，另一个随机变量的不确定性。

#### 数学定义
$Y$ 的条件熵是指在随机变量 $X$ 发生的前提下，随机变量 $Y$ 发生新带来的熵：

$$H(Y|X) = -\sum_{x \in X} \sum_{y \in Y} P(x,y) \log P(y|x)$$


### 相对熵
**Kullback–Leibler Divergence(KL散度)**

#### 概念

相对熵（KL散度）用于衡量两个概率分布的差异程度。
它描述了当我们用概率分布Q来拟合真实分布P时，产生的信息损耗。


#### 数学定义
**Mathematical Definition**

对于两个概率分布 $P$ 和 $Q$，其相对熵定义为：

$$D_{KL}(P||Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}$$


### 交叉熵
**Cross Entropy**

#### 概念
**Concept**

交叉熵用于度量两个概率分布间的差异性，在机器学习中常用作损失函数。
它衡量了使用概率分布Q来表示真实分布P时所需的平均比特数。

#### 数学定义

$$H(P,Q) = -\sum_{i} P(i) \log Q(i)$$

#### 与相对熵的关系
交叉熵可以看作是相对熵（KL散度）加上熵：
$$D_{KL}(P||Q) = H(P,Q) - H(P)$$

其中：
- $H(P,Q)$ 是交叉熵
- $H(P)$ 是真实分布P的熵
- $D_{KL}(P||Q)$ 是相对熵

#### 应用
1. 分类任务
   - 二分类：二元交叉熵
   - 多分类：多元交叉熵
   - 序列分类：序列交叉熵

2. 为什么选择交叉熵作为损失函数
   - 真实概率分布P是确定的，因此熵H(P)是一个常量
   - 最小化交叉熵等价于最小化相对熵
   - 交叉熵的计算公式更简单，数值稳定性更好
   - 梯度计算更加直接，有利于反向传播

3. 优化特性
   - 非负性：$H(P,Q) \geq 0$
   - 当P=Q时取得最小值
   - 对异常值不敏感
   - 梯度计算简单，有利于模型训练

<br>

### 互信息
**Mutual Information**

#### 概念

互信息描述两个随机变量之间的相互依赖程度。
互信息是信息论里一种有用的信息度量方式，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。

#### 数学定义

$$I(X;Y) = \sum_{x \in X} \sum_{y \in Y} P(x,y) \log \frac{P(x,y)}{P(x)P(y)}$$

<br>

### 最大熵原理
**Maximum Entropy Principle**


#### 数学定义
**Mathematical Expression**

对于随机变量 $X$ 的概率分布 $P(X)$，其熵满足：

$$H(X) \leq \log |X|$$

当且仅当 $X$ 服从均匀分布时等号成立。

#### 应用


**Machine Learning**：

1. 损失函数
   - 交叉熵损失
   - KL散度损失
   - 互信息最大化

2. 特征选择
   - 信息增益
   - 互信息特征选择
   - 最大相关最小冗余

**Deep Learning**：

1. 信息瓶颈理论
   - 压缩与预测的权衡
   - 表示学习
   - 模型解释性

2. 生成模型
   - 变分自编码器
   - 生成对抗网络
   - 扩散模型

### 总结

| 概念 | 英文 | 定义 | 物理意义 | 应用场景 |
|------|------|------|----------|----------|
| 熵 | Entropy | $H(X) = -\sum p_i \log p_i$ | 随机变量的不确定度 | 信息压缩、特征选择 |
| 联合熵 | Joint Entropy | $H(X,Y) = -\sum P(x,y) \log P(x,y)$ | 多随机变量系统的不确定度 | 多变量分析 |
| 条件熵 | Conditional Entropy | $H(Y\|X) = -\sum P(x,y) \log P(y\|x)$ | 已知X条件下Y的不确定度 | 条件概率建模 |
| 相对熵 | KL Divergence | $D_{KL}(P\|\|Q) = \sum P(i) \log \frac{P(i)}{Q(i)}$ | 两个概率分布的差异 | 模型评估、变分推断 |
| 交叉熵 | Cross Entropy | $H(P,Q) = -\sum P(i) \log Q(i)$ | 两个概率分布间的差异 | 分类任务损失函数 |
| 互信息 | Mutual Information | $I(X;Y) = \sum P(x,y) \log \frac{P(x,y)}{P(x)P(y)}$ | 变量间的相互依赖程度 | 特征选择、表示学习 |